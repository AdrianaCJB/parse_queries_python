{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levantamiento Acciones Comerciales - Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **PROYECTO : LEVANTAMIENTO CÓDIGO EN EL GESTOR DE CAMPANAS TERADATA** <br> \n",
    "**Extracción de tablas, campos y reglas utilizadas en las Vistas, Bteq, SP e Input Analítico** <br>\n",
    "Los archivos a procesar están en la carpeta ./ArchivosProcesar  <br>\n",
    "Versión:  1.0  <br>\n",
    "Fecha: 01-05-2020  <br>\n",
    "Descripción: Versión Inicial  <br>\n",
    "Desarrollador: Axity | Adriana Jiménez "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10482,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "import os, glob\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from decimal import Decimal as D\n",
    "import datetime\n",
    "\n",
    "## Librerias de parseo de queries\n",
    "import sqlparse\n",
    "from sqlparse.sql import TokenList\n",
    "from sqlparse.tokens import Name, Whitespace, Wildcard, Number, Punctuation, Text, Operator, Literal\n",
    "from sqlparse.tokens import DML, DDL, Keyword\n",
    "import sql_metadata as sqllib\n",
    "\n",
    "\n",
    "## Libreria que se integra con Teradata\n",
    "import giraffez\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10483,
   "metadata": {},
   "outputs": [],
   "source": [
    "noConditionInput = 'Parametros: No tiene condicion de tabla input'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(_list):\n",
    "    \"\"\"\n",
    "    Hace que una lista tenga registro unicos y mantengan el orden\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "\n",
    "    for item in _list:\n",
    "        if item not in ret:\n",
    "            ret.append(item)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones: Insertar en BD Teradata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10485,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuracion de conexion\n",
    "td_config = {\n",
    "    \"username\": \"exajibl\",\n",
    "    \"password\": \"acjb0610\",\n",
    "    \"host\": \"dataware.bci.cl\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_drop_tables_teradata():\n",
    "    '''\n",
    "    Función que crea las tablas en Teradata\n",
    "    '''\n",
    "    \n",
    "    print(\"ELIMINANDO Y CREANDO TABLAS..\")\n",
    "    drop_sql_bteq    = \"DROP TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_BTEQ\"    \n",
    "    drop_sql_sp      = \"DROP TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_SP\"\n",
    "    drop_sql_vistas  = \"DROP TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_VISTAS\"\n",
    "    \n",
    "        \n",
    "    create_sql_bteq = \"\"\"CREATE MULTISET TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_BTEQ\n",
    "        (\n",
    "          TIPO_ARCHIVO VARCHAR(20) ,\n",
    "          ARCHIVO VARCHAR(200),\n",
    "          LINEAS_DE_CODIGO VARCHAR(100),\n",
    "          NUMERO_PASO VARCHAR(10),      \n",
    "          SENTENCIA_DML VARCHAR(15) ,\n",
    "          ESQUEMA_OUTPUT VARCHAR(100) ,\n",
    "          TABLA_OUTPUT VARCHAR(100) ,\n",
    "          MULTINIVEL VARCHAR(30),\n",
    "          ESQUEMA_INPUT VARCHAR(100) ,\n",
    "          TABLA_INPUT VARCHAR(100) ,\n",
    "          ALIAS_SUBQUERY VARCHAR(100) ,\n",
    "          COLUMNA VARCHAR(100) ,\n",
    "          VARIABLE VARCHAR(100) ,\n",
    "          LOGICA_NEGOCIO VARCHAR(28000),\n",
    "          ALIAS_CAMPO VARCHAR(100)   \n",
    "          ) ;\"\"\"       \n",
    "   \n",
    "    create_sql_sp = \"\"\"CREATE MULTISET TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_SP\n",
    "        (\n",
    "          TIPO_ARCHIVO VARCHAR(20) ,\n",
    "          ARCHIVO VARCHAR(200),\n",
    "          LINEAS_DE_CODIGO VARCHAR(100),          \n",
    "          NUMERO_PASO VARCHAR(10),      \n",
    "          SENTENCIA_DML VARCHAR(15) ,\n",
    "          ESQUEMA_OUTPUT VARCHAR(100) ,\n",
    "          TABLA_OUTPUT VARCHAR(100) ,\n",
    "          MULTINIVEL VARCHAR(30),\n",
    "          ESQUEMA_INPUT VARCHAR(100) ,\n",
    "          TABLA_INPUT VARCHAR(100) ,\n",
    "          ALIAS_SUBQUERY VARCHAR(100) ,\n",
    "          COLUMNA VARCHAR(100) ,\n",
    "          VARIABLE VARCHAR(100) ,\n",
    "          LOGICA_NEGOCIO VARCHAR(28000),\n",
    "          ALIAS_CAMPO VARCHAR(100)      \n",
    "          ) ;\"\"\"\n",
    "    \n",
    "    create_sql_vistas = \"\"\"CREATE MULTISET TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_VISTAS\n",
    "        (\n",
    "          TIPO_ARCHIVO VARCHAR(20) ,\n",
    "          ARCHIVO VARCHAR(200),   \n",
    "          LINEAS_DE_CODIGO VARCHAR(100),  \n",
    "          SENTENCIA_DML VARCHAR(15) ,\n",
    "          ESQUEMA_OUTPUT VARCHAR(30) ,\n",
    "          TABLA_OUTPUT VARCHAR(50) ,\n",
    "          MULTINIVEL VARCHAR(30),\n",
    "          ESQUEMA_INPUT VARCHAR(50) ,\n",
    "          TABLA_INPUT VARCHAR(50),\n",
    "          ALIAS_SUBQUERY VARCHAR(50) ,\n",
    "          COLUMNA VARCHAR(50) ,\n",
    "          VARIABLE VARCHAR(50) ,\n",
    "          LOGICA_NEGOCIO VARCHAR(28000),\n",
    "          ALIAS_CAMPO VARCHAR(50)        \n",
    "          ) ;\"\"\"\n",
    "    \n",
    "    with giraffez.Cmd(**td_config) as cmd:\n",
    "        if cmd.exists(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_BTEQ\"):\n",
    "            cmd.execute(drop_sql_bteq)\n",
    "        if cmd.exists(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_SP\"):\n",
    "            cmd.execute(drop_sql_sp)\n",
    "        if cmd.exists(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_VISTAS\"):\n",
    "            cmd.execute(drop_sql_vistas)\n",
    "    \n",
    "\n",
    "        cmd.execute(create_sql_bteq)\n",
    "        cmd.execute(create_sql_sp)\n",
    "        cmd.execute(create_sql_vistas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_files_csv_in_teradata():\n",
    "    '''\n",
    "    Función que inserta los dataframes a Teradata\n",
    "    '''\n",
    "    \n",
    "    print(\"INSERTANDO ARCHIVOS A LAS TABLAS..\\n\")\n",
    "    \n",
    "    df_bteq    = pd.read_csv(\"LAC_Levantamiento_Bteq.csv\",  sep='|' ) \n",
    "    df_vistas  = pd.read_csv(\"LAC_Levantamiento_Vistas.csv\",  sep='|') \n",
    "    df_sp      = pd.read_csv(\"LAC_Levantamiento_StoredProcedures.csv\" ,  sep='|') \n",
    "\n",
    "    \n",
    "    if (df_bteq.empty == False):         \n",
    "        \n",
    "        print(\"INSERTANDO EN EDW_TEMPUSU.LAC_LEVANTAMIENTO_BTEQ\")\n",
    "        \n",
    "        df_bteq['TIPO_ARCHIVO']    = df_bteq['TIPO_ARCHIVO'].astype('str')\n",
    "        df_bteq['ARCHIVO']         = df_bteq['ARCHIVO'].astype('str')            \n",
    "        df_bteq['LINEAS_DE_CODIGO'] = df_bteq['LINEAS_DE_CODIGO'].astype('str')    \n",
    "        df_bteq['NUMERO_PASO']     = df_bteq['NUMERO_PASO'].astype('str')\n",
    "        df_bteq['SENTENCIA_DML']   = df_bteq['SENTENCIA_DML'].astype('str')\n",
    "        df_bteq['ESQUEMA_OUTPUT']  = df_bteq['ESQUEMA_OUTPUT'].astype('str')\n",
    "        df_bteq['TABLA_OUTPUT']    = df_bteq['TABLA_OUTPUT'].astype('str')\n",
    "        df_bteq['MULTINIVEL']      = df_bteq['MULTINIVEL'].astype('str')\n",
    "        df_bteq['ESQUEMA_INPUT']   = df_bteq['ESQUEMA_INPUT'].astype('str')\n",
    "        df_bteq['TABLA_INPUT']     = df_bteq['TABLA_INPUT'].astype('str')\n",
    "        df_bteq['ALIAS_SUBQUERY']  = df_bteq['ALIAS_SUBQUERY'].astype('str')\n",
    "        df_bteq['COLUMNA']         = df_bteq['COLUMNA'].astype('str')\n",
    "        df_bteq['VARIABLE']        = df_bteq['VARIABLE'].astype('str')\n",
    "        df_bteq['LOGICA_NEGOCIO']  = df_bteq['LOGICA_NEGOCIO'].astype('str')\n",
    "        df_bteq['ALIAS_CAMPO']     = df_bteq['ALIAS_CAMPO'].astype('str')\n",
    "        \n",
    "        \n",
    "        with giraffez.BulkLoad(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_BTEQ\", **td_config) as load:\n",
    "            load.cleanup()\n",
    "            load.columns = df_bteq.columns.tolist()\n",
    "            for row in df_bteq.values.tolist(): \n",
    "                load.put([row[0], row[1], row[2], row[3], row[4], row[5], row[6],\n",
    "                          row[7], row[8], row[9], row[10], row[11], row[12], row[13], row[14] ])\n",
    "        \n",
    "\n",
    "    if (df_sp.empty == False):\n",
    "        \n",
    "        print(\"INSERTANDO EN EDW_TEMPUSU.LAC_LEVANTAMIENTO_SP\")\n",
    "        \n",
    "        df_sp['TIPO_ARCHIVO']    = df_sp['TIPO_ARCHIVO'].astype('str')\n",
    "        df_sp['ARCHIVO']         = df_sp['ARCHIVO'].astype('str')   \n",
    "        df_sp['LINEAS_DE_CODIGO']= df_sp['LINEAS_DE_CODIGO'].astype('str')\n",
    "        df_sp['NUMERO_PASO']     = df_sp['NUMERO_PASO'].astype('str')\n",
    "        df_sp['SENTENCIA_DML']   = df_sp['SENTENCIA_DML'].astype('str')\n",
    "        df_sp['ESQUEMA_OUTPUT']  = df_sp['ESQUEMA_OUTPUT'].astype('str')\n",
    "        df_sp['TABLA_OUTPUT']    = df_sp['TABLA_OUTPUT'].astype('str')\n",
    "        df_sp['MULTINIVEL']      = df_sp['MULTINIVEL'].astype('str')\n",
    "        df_sp['ESQUEMA_INPUT']   = df_sp['ESQUEMA_INPUT'].astype('str')\n",
    "        df_sp['TABLA_INPUT']     = df_sp['TABLA_INPUT'].astype('str')\n",
    "        df_sp['ALIAS_SUBQUERY']  = df_sp['ALIAS_SUBQUERY'].astype('str')\n",
    "        df_sp['COLUMNA']         = df_sp['COLUMNA'].astype('str')\n",
    "        df_sp['VARIABLE']        = df_sp['VARIABLE'].astype('str')\n",
    "        df_sp['LOGICA_NEGOCIO']  = df_sp['LOGICA_NEGOCIO'].astype('str')\n",
    "        df_sp['ALIAS_CAMPO']     = df_sp['ALIAS_CAMPO'].astype('str')\n",
    "        \n",
    "        \n",
    "        with giraffez.BulkLoad(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_SP\", **td_config) as load:\n",
    "            load.cleanup()\n",
    "            load.columns = df_sp.columns.tolist()\n",
    "            for row in df_sp.values.tolist(): \n",
    "                load.put([row[0], row[1], row[2], row[3], row[4], row[5], row[6],\n",
    "                          row[7], row[8], row[9], row[10], row[11], row[12], row[13], row[14] ])  \n",
    "        \n",
    "        \n",
    "    \n",
    "    if (df_vistas.empty == False): \n",
    "        \n",
    "        print(\"INSERTANDO EN EDW_TEMPUSU.LAC_LEVANTAMIENTO_VISTAS\")\n",
    "        \n",
    "        df_vistas['TIPO_ARCHIVO']    = df_vistas['TIPO_ARCHIVO'].astype('str')\n",
    "        df_vistas['ARCHIVO']         = df_vistas['ARCHIVO'].astype('str')\n",
    "        df_vistas['LINEAS_DE_CODIGO']= df_vistas['LINEAS_DE_CODIGO'].astype('str')\n",
    "        df_vistas['SENTENCIA_DML']   = df_vistas['SENTENCIA_DML'].astype('str')\n",
    "        df_vistas['ESQUEMA_OUTPUT']  = df_vistas['ESQUEMA_OUTPUT'].astype('str')\n",
    "        df_vistas['TABLA_OUTPUT']    = df_vistas['TABLA_OUTPUT'].astype('str')\n",
    "        df_vistas['MULTINIVEL']      = df_vistas['MULTINIVEL'].astype('str')\n",
    "        df_vistas['ESQUEMA_INPUT']   = df_vistas['ESQUEMA_INPUT'].astype('str')\n",
    "        df_vistas['TABLA_INPUT']     = df_vistas['TABLA_INPUT'].astype('str')\n",
    "        df_vistas['ALIAS_SUBQUERY']  = df_vistas['ALIAS_SUBQUERY'].astype('str')\n",
    "        df_vistas['COLUMNA']         = df_vistas['COLUMNA'].astype('str')\n",
    "        df_vistas['VARIABLE']        = df_vistas['VARIABLE'].astype('str')\n",
    "        df_vistas['LOGICA_NEGOCIO']  = df_vistas['LOGICA_NEGOCIO'].astype('str')\n",
    "        df_vistas['ALIAS_CAMPO']     = df_vistas['ALIAS_CAMPO'].astype('str')\n",
    "        \n",
    "        \n",
    "        with giraffez.BulkLoad(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_VISTAS\", **td_config) as load:\n",
    "            load.cleanup()\n",
    "            load.columns = df_vistas.columns.tolist()\n",
    "            for row in df_vistas.values.tolist(): \n",
    "                load.put([row[0], row[1], row[2], row[3], row[4], row[5], row[6],\n",
    "                          row[7], row[8], row[9], row[10],row[11], row[12], row[13]  ])\n",
    "           \n",
    "              \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de procesamiento de query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10488,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comments(text):\n",
    "    \"\"\" remove c-style comments.\n",
    "        text: blob of text with comments (can include newlines)\n",
    "        returns: text with comments removed\n",
    "    \"\"\"\n",
    "    pattern = r\"\"\"\n",
    "                            ##  --------- COMMENT ---------\n",
    "           /\\*              ##  Start of /* ... */ comment\n",
    "           [^*]*\\*+         ##  Non-* followed by 1-or-more *'s\n",
    "           (                ##\n",
    "             [^/*][^*]*\\*+  ##\n",
    "           )*               ##  0-or-more things which don't start with /\n",
    "                            ##    but do end with '*'\n",
    "           /                ##  End of /* ... */ comment\n",
    "         |                  ##  -OR-  various things which aren't comments:\n",
    "           (                ## \n",
    "                            ##  ------ \" ... \" STRING ------\n",
    "             \"              ##  Start of \" ... \" string\n",
    "             (              ##\n",
    "               \\\\.          ##  Escaped char\n",
    "             |              ##  -OR-\n",
    "               [^\"\\\\]       ##  Non \"\\ characters\n",
    "             )*             ##\n",
    "             \"              ##  End of \" ... \" string\n",
    "           |                ##  -OR-\n",
    "                            ##\n",
    "                            ##  ------ ' ... ' STRING ------\n",
    "             '              ##  Start of ' ... ' string\n",
    "             (              ##\n",
    "               \\\\.          ##  Escaped char\n",
    "             |              ##  -OR-\n",
    "               [^'\\\\]       ##  Non '\\ characters\n",
    "             )*             ##\n",
    "             '              ##  End of ' ... ' string\n",
    "           |                ##  -OR-\n",
    "                            ##\n",
    "                            ##  ------ ANYTHING ELSE -------\n",
    "             .              ##  Anything other char\n",
    "             [^/\"'\\\\]*      ##  Chars which doesn't start a comment, string\n",
    "           )                ##    or escape\n",
    "    \"\"\"\n",
    "    regex = re.compile(pattern, re.VERBOSE|re.MULTILINE|re.DOTALL)\n",
    "    noncomments = [m.group(2) for m in regex.finditer(text) if m.group(2)]\n",
    "\n",
    "    return \"\".join(noncomments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteInconsistencyQuery(queryString):\n",
    "    '''\n",
    "    Función que elimina caracteres varios de una query\n",
    "    '''    \n",
    "    characters = '[%#${}]'\n",
    "    finalQuery = re.sub(characters, '', str(queryString))\n",
    "    return finalQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10490,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(queryString):\n",
    "    '''\n",
    "    Función que limpia y ordena una query para ser procesada posteriormente\n",
    "    '''\n",
    "    queryString = queryString.replace('\"','')\n",
    "    queryString = sqllib.preprocess_query(queryString)\n",
    "    queryString = deleteInconsistencyQuery(queryString)\n",
    "    return queryString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10491,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeDataframeTablesColumnsQuery(df_final_tables, df_final_columns):\n",
    "    '''\n",
    "    Función que une las tablas con los campos según el multinivel al que corresponde\n",
    "    '''\n",
    "    \n",
    "    columnsDF = ['MULTINIVEL','ESQUEMA_INPUT','TABLA_INPUT','ALIAS_SUBQUERY',\n",
    "                 'NOMBRE_CAMPO', 'CAMPO_COMPUESTO','LOGICA_NEGOCIO','ALIAS_CAMPO']\n",
    "    \n",
    "    \n",
    "    # Divide la columna \"CAMPOS\" haciendo split '.'\n",
    "    # Obtengo un ALIAS temporal del CAMPO\n",
    "    \n",
    "    df_tmp_columns = df_final_columns.CAMPOS.str.split(\".\", n=3, expand=True)\n",
    "\n",
    "    \n",
    "    if (df_tmp_columns.empty == False):\n",
    "        # Para los casos que viene el campo en formato: esquema.tabla.campo\n",
    "        if (len(df_tmp_columns.columns) == 3): \n",
    "            df_final_columns['ALIAS_TMP']  = 'N/A'\n",
    "            df_final_columns[['COLUMNA_1','COLUMNA_2','NOMBRE_CAMPO']] = \\\n",
    "                                df_final_columns.CAMPOS.str.split(\".\", n=3, expand=True)\n",
    "\n",
    "            for index, row in df_final_columns.iterrows(): \n",
    "                if ((row[\"NOMBRE_CAMPO\"] == None or row[\"NOMBRE_CAMPO\"] == np.NaN) \\\n",
    "                and (row[\"COLUMNA_2\"]    == None or row[\"COLUMNA_2\"]    == np.NaN) ):\n",
    "                    row[\"NOMBRE_CAMPO\"] = row[\"COLUMNA_1\"]                 \n",
    "\n",
    "                elif (row[\"NOMBRE_CAMPO\"] == None or row[\"NOMBRE_CAMPO\"] == np.NaN ):\n",
    "                    row[\"NOMBRE_CAMPO\"] = row[\"COLUMNA_2\"] \n",
    "                    row[\"ALIAS_TMP\"]    = row[\"COLUMNA_1\"] \n",
    "\n",
    "\n",
    "        # Para los casos que viene el campo en formato: alias.campo    \n",
    "        if (len(df_tmp_columns.columns) == 2):    \n",
    "            df_final_columns[['ALIAS_TMP','NOMBRE_CAMPO']] = df_final_columns.CAMPOS.str.split(\".\", n=2, expand=True)\n",
    "\n",
    "            for index, row in df_final_columns.iterrows(): \n",
    "                if (row[\"NOMBRE_CAMPO\"] == None or row[\"NOMBRE_CAMPO\"] == np.NaN ):\n",
    "                    row[\"NOMBRE_CAMPO\"] = row[\"ALIAS_TMP\"] \n",
    "                    row[\"ALIAS_TMP\"]    = 'N/A'\n",
    "\n",
    "\n",
    "        # Para los casos que viene el campo en formato: campo   \n",
    "        elif (len(df_tmp_columns.columns) == 1): \n",
    "            df_final_columns['ALIAS_TMP']  = 'N/A'\n",
    "            df_final_columns['NOMBRE_CAMPO'] = df_final_columns.CAMPOS.str.split(\".\", n=1, expand=True)\n",
    "\n",
    "\n",
    "            \n",
    "        df_final_columns['ALIAS_TMP']  = df_final_columns['ALIAS_TMP'].str.upper() \n",
    "        df_final_tables['ALIAS_TABLA'] = df_final_tables['ALIAS_TABLA'].str.upper() \n",
    "\n",
    "        # Merge para los match completos entre campos y tablas (INNER JOIN)  \n",
    "        df_merge = pd.merge(df_final_columns, df_final_tables, \\\n",
    "                            left_on  =[\"MULTINIVEL\",\"ALIAS_TMP\"], right_on =[\"MULTINIVEL\",\"ALIAS_TABLA\"], \\\n",
    "                            how ='outer')\n",
    "\n",
    "        \n",
    "        # Los casos en que las subquery tiene una sola tabla y los campos no tienen esquema, asignar esa tabla\n",
    "        # Cuento la cantidad de tablas que tiene un multinivel\n",
    "        df_count_tables = df_final_tables.groupby(['MULTINIVEL'])['ESQUEMA_INPUT','TABLA_INPUT'].count()\n",
    "\n",
    "        # Obtengo los multiniveles que solo tienen 1 tabla \n",
    "        df_count_tables = df_count_tables[(df_count_tables['ESQUEMA_INPUT'] == 1) \\\n",
    "                                        & (df_count_tables['TABLA_INPUT'] == 1)].reset_index()\n",
    "        df_count_tables = df_count_tables[['MULTINIVEL']]\n",
    "\n",
    "        # Hago merge con el dataframe anterior para obtener de nuevo los nombres\n",
    "        df_merge_count = pd.merge(df_final_tables, df_count_tables, on  =[\"MULTINIVEL\"], how ='inner')\n",
    "\n",
    "\n",
    "        # Asigna a los campos que no tienen match, su tabla correspondiente, en los casos de 1 sola tabla\n",
    "        for index, row in df_merge_count.iterrows(): \n",
    "            df_merge['ESQUEMA_INPUT'] = np.where((df_merge['MULTINIVEL'] == row['MULTINIVEL']) & \\\n",
    "                                                (df_merge['ALIAS_TMP'] == 'N/A'), \\\n",
    "                                                row[\"ESQUEMA_INPUT\"], df_merge[\"ESQUEMA_INPUT\"] )        \n",
    "\n",
    "            df_merge['TABLA_INPUT'] = np.where((df_merge['MULTINIVEL'] == row['MULTINIVEL']) & \\\n",
    "                                              (df_merge['ALIAS_TMP'] == 'N/A'), \\\n",
    "                                              row[\"TABLA_INPUT\"], df_merge[\"TABLA_INPUT\"] )\n",
    "\n",
    "            df_merge['ALIAS_TABLA'] = np.where((df_merge['MULTINIVEL'] == row['MULTINIVEL']) & \\\n",
    "                                           (df_merge['ALIAS_TMP'] == 'N/A'), \\\n",
    "                                           row[\"ALIAS_TABLA\"], df_merge[\"ALIAS_TABLA\"] )\n",
    "\n",
    "            df_merge['ALIAS_SUBQUERY'] = np.where((df_merge['MULTINIVEL'] == row['MULTINIVEL']) & \\\n",
    "                                           (df_merge['ALIAS_TMP'] == 'N/A'), \\\n",
    "                                           row[\"ALIAS_SUBQUERY\"], df_merge[\"ALIAS_SUBQUERY\"] )\n",
    "\n",
    "\n",
    "        # Genero un campo compuesto con el formato esquema.tabla.campo \n",
    "        cols = ['ESQUEMA_INPUT', 'TABLA_INPUT', 'NOMBRE_CAMPO']\n",
    "        df_merge['CAMPO_COMPUESTO'] = df_merge[cols].apply(lambda row: np.nan \\\n",
    "                    if (row['NOMBRE_CAMPO'] is np.nan or row['NOMBRE_CAMPO'] == None) \\\n",
    "                    else '.'.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "        # Obtego las columnas que quiero del df \n",
    "        df_merge_final = df_merge[columnsDF] \n",
    "\n",
    "    \n",
    "    return df_merge_final\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertDataframeTablesQuery(df_total_tables, multilevel, table_list, subquery_alias):\n",
    "    '''\n",
    "    Función que inserta en un dataframe las tablas de una query\n",
    "    '''\n",
    "    new_df_tables       = emptyDataframeTablesQuery(None)\n",
    "    \n",
    "    #print (\" ****** LISTA TABLAS  -> \"          + str(table_list)) \n",
    "\n",
    "    ## SECCION DE INSERCION DE TABLAS SOLAMENTE\n",
    "    for item in table_list:\n",
    "        tamañoItem = len(item)\n",
    "        \n",
    "        if (tamañoItem == 3):\n",
    "            new_df_tables = new_df_tables.append(\n",
    "                [ {'MULTINIVEL': multilevel ,\n",
    "                   'ESQUEMA_INPUT': item[0],\n",
    "                   'TABLA_INPUT': item[1], \n",
    "                   'ALIAS_TABLA' : item[2] }], \n",
    "                    ignore_index=True, sort=False) \n",
    "                           \n",
    "        elif (tamañoItem == 2):\n",
    "            new_df_tables = new_df_tables.append(\n",
    "                [ {'MULTINIVEL': multilevel ,\n",
    "                   'ESQUEMA_INPUT': item[0],\n",
    "                   'TABLA_INPUT': item[1] }], \n",
    "                    ignore_index=True, sort=False) \n",
    "        \n",
    "    \n",
    "    new_df_tables = new_df_tables.assign( ALIAS_SUBQUERY = subquery_alias )   \n",
    "    df_total_tables = df_total_tables.append(new_df_tables)\n",
    "        \n",
    "    return df_total_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertDataframeColumnsQuery(df_total_columns, multilevel, column_list, column_list_total,\n",
    "            rule_select_list_total, rule_generic_list_total):\n",
    "    '''\n",
    "    Función que inserta en un dataframe los campos, alias, reglas y filtros de una query\n",
    "    '''\n",
    "    \n",
    "    new_df_column       = emptyDataframeColumnsQuery(None)\n",
    "    new_df_internal     = emptyDataframeColumnsQuery(None)\n",
    "    \n",
    "    # Lista los campos que tuvieron logica asociada\n",
    "    check_rule_columns  = []\n",
    "    \n",
    "    # Lista temporal de campos y reglas\n",
    "    list_columns_rules  = []\n",
    "    alias               = ''\n",
    "    flag_column_with_as = False  # Se activa si consigue que un campo contiene alias\n",
    "    column_list = unique(column_list) \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    print (\" ****** POSICION DE MULTINIVEL -> \" + str(multilevel))\n",
    "    print (\" ****** LISTA CAMPOS  -> \"          + str(column_list))\n",
    "    print (\" ****** LISTA CAMPOS TOTAL  -> \"    + str(column_list_total))\n",
    "    print (\" ****** LISTA LOGICAS SELECT -> \"   + str(rule_select_list_total)) \n",
    "    print (\" ****** LISTA LOGICAS FILTROS -> \"  + str(rule_generic_list_total)) \n",
    "    '''\n",
    "    \n",
    "    ## Recorre la lista de las reglas y si existe un campo que esté contenido en la regla\n",
    "    ## entonces inserta en la misma fila el campo y la regla interna.      \n",
    "    if (rule_select_list_total):\n",
    "        \n",
    "        ## Logicas de los 'select'\n",
    "        for rule in rule_select_list_total:\n",
    "            flag_column_rule = False\n",
    "            \n",
    "            # Recorre cada campo para hacer match con la regla/logica\n",
    "            for column in column_list:                 \n",
    "                if (column in rule[0] and column != rule[1]):                  \n",
    "                    flag_column_rule = True\n",
    "                    new_df_column = new_df_column.append(\n",
    "                    [{  'MULTINIVEL': multilevel , \n",
    "                        'CAMPOS': column,\n",
    "                        'LOGICA_NEGOCIO' : rule[0],\n",
    "                        'ALIAS_CAMPO'   : rule[1]\n",
    "                    }], \n",
    "                     ignore_index=True, sort=False)                 \n",
    "                    check_rule_columns.append(column)\n",
    "                    new_df_internal = new_df_internal.append(new_df_column,ignore_index=True, sort=False)\n",
    "                    \n",
    "                    \n",
    "            ## Si la regla no tiene campo, insertarla de igual forma        \n",
    "            if (flag_column_rule == False):\n",
    "                new_df_column = new_df_column.append(\n",
    "                [{  'MULTINIVEL': multilevel ,\n",
    "                    'CAMPOS': None,\n",
    "                    'LOGICA_NEGOCIO' : rule[0],\n",
    "                    'ALIAS_CAMPO'   : rule[1]\n",
    "                }], \n",
    "                 ignore_index=True, sort=False) \n",
    "                new_df_internal = new_df_internal.append(new_df_column,ignore_index=True, sort=False)\n",
    "         \n",
    "    ## Logicas de negocio de los filtros\n",
    "    if (rule_generic_list_total):\n",
    "        for rule in rule_generic_list_total:\n",
    "            for column in column_list: \n",
    "                if (column in rule):\n",
    "                    new_df_column = new_df_column.append(\n",
    "                    [{  'MULTINIVEL': multilevel ,\n",
    "                        'CAMPOS': column,\n",
    "                        'LOGICA_NEGOCIO' : rule}], \n",
    "                     ignore_index=True, sort=False) \n",
    "                    \n",
    "                    check_rule_columns.append(column)\n",
    "                    new_df_internal = new_df_internal.append(new_df_column,ignore_index=True, sort=False)\n",
    "            \n",
    "    \n",
    "    ## los campos solos que no tengan regla asociada, se insertan al final\n",
    "    ## pero hay qe validar si tienen alias también\n",
    "    columns_without_rules = list(set(column_list) - set(check_rule_columns))        \n",
    "    \n",
    "    \n",
    "    if (columns_without_rules):\n",
    "        for column_w_r in columns_without_rules:  \n",
    "            flag_column_with_as = False\n",
    "            \n",
    "            if (column_list_total):\n",
    "                for column_as in column_list_total:\n",
    "                    \n",
    "                    field = column_as[0]\n",
    "                    len_list_field = len(column_as)\n",
    "                    \n",
    "                    if (len_list_field == 2):\n",
    "                        if (field != None and column_w_r == field):\n",
    "                            flag_column_with_as = True\n",
    "                            alias = column_as[1]                            \n",
    "                            new_df_column = new_df_column.append(\n",
    "                                        [{  'MULTINIVEL': multilevel ,\n",
    "                                            'CAMPOS': field,\n",
    "                                            'ALIAS_CAMPO': alias \n",
    "                                         }], \n",
    "                                         ignore_index=True, sort=False)  \n",
    "                            \n",
    "            ## Si el campo de la lista de campos sin reglas no tiene alias, insertar al final\n",
    "            if (flag_column_with_as == False):           \n",
    "                new_df_column = new_df_column.append(\n",
    "                            [{  'MULTINIVEL': multilevel ,\n",
    "                                'CAMPOS': column_w_r,\n",
    "                                'ALIAS_CAMPO': None \n",
    "                             }], \n",
    "                             ignore_index=True, sort=False) \n",
    "            \n",
    "            new_df_internal = new_df_internal.append(new_df_column,ignore_index=True, sort=False)\n",
    "    \n",
    "            \n",
    "    if (new_df_internal.empty == False):        \n",
    "        new_df_internal  = new_df_internal.drop_duplicates()    \n",
    "        new_df_internal  = new_df_internal.sort_values(by ='CAMPOS')\n",
    "        df_total_columns = df_total_columns.append(new_df_internal,ignore_index=True, sort=False)\n",
    "        \n",
    "    \n",
    "    return df_total_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10494,
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateInDataFrameColumnsRules(df_tables_columns, rule_generic_list_total):\n",
    "    '''Función que actualiza o inserta en los campos las ultimas reglas encontradas en la query.\n",
    "       Es decir, multiquery = 1 y antes del final con ; \n",
    "    '''\n",
    "    last_rules    = []\n",
    "    internal_rule = []\n",
    "    \n",
    "        \n",
    "    if (rule_generic_list_total and df_tables_columns.empty == False):\n",
    "        for rule in rule_generic_list_total: \n",
    "            \n",
    "            # Si el campo No tiene lógica de negocio o regla, se actualiza el df\n",
    "            df_tables_columns['LOGICA_NEGOCIO'] = \\\n",
    "                df_tables_columns.apply(lambda df: rule \\\n",
    "                   if (str(df['CAMPOS']) in rule and df['LOGICA_NEGOCIO'] is np.nan) \\\n",
    "                    else df['LOGICA_NEGOCIO'], axis=1)\n",
    "\n",
    "\n",
    "            # Si el campo Si tiene lógica de negocio o regla diferente a la insertada,\n",
    "            # se inserta el nuevo al df            \n",
    "            for index, row in df_tables_columns.iterrows(): \n",
    "                if (row[\"MULTINIVEL\"] == '1' and \\\n",
    "                    str(row[\"CAMPOS\"]) in rule and \\\n",
    "                    row[\"LOGICA_NEGOCIO\"] != rule ):\n",
    "\n",
    "                    internal_rule = [row[\"MULTINIVEL\"], row[\"CAMPOS\"], rule]\n",
    "                    last_rules.append(internal_rule)\n",
    "            \n",
    "    last_rules = unique(last_rules)    \n",
    "    new_dataframe = pd.DataFrame(last_rules, columns = ['MULTINIVEL','CAMPOS','LOGICA_NEGOCIO'] )\n",
    "    \n",
    "    df_tables_columns = df_tables_columns.append(new_dataframe, ignore_index=True)\n",
    "    \n",
    "    return df_tables_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertaEnArchivoDF(file, df):\n",
    "    '''\n",
    "    Función que inserta un dataframe a un archivo\n",
    "    '''\n",
    "    df.to_csv(file, header=True, index=None, sep='|', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10496,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables_output(query):\n",
    "    ''' \n",
    "    Recorre los tokens de 'INSERT, REPLACE VIEW' hasta conseguir un 'SELECT'\n",
    "    Retorna lista: [tipo de stmt, esquema output, tabla output]\n",
    "    '''\n",
    "    list_output = []\n",
    "    create_seen         = False\n",
    "    replace_seen        = False\n",
    "    rename_seen         = False\n",
    "    drop_seen           = False\n",
    "    last_token          = None\n",
    "\n",
    "    for token in sqllib.get_query_tokens(query):\n",
    "        \n",
    "        #print(str(token.ttype) + \" --> \" + str(token.value.upper()))\n",
    "        \n",
    "        if (token.ttype is DML and token.value.upper() == 'SELECT') \\\n",
    "        or (token.ttype is Punctuation and token.value in ['(',';']) \\\n",
    "        or (token.ttype is Keyword and token.value == 'VALUES') \\\n",
    "        or (len(list_output) == 3):\n",
    "            \n",
    "            #print(list_output)\n",
    "            return list_output\n",
    "            \n",
    "        elif token.ttype is DML and \\\n",
    "            token.value.upper() in ['INSERT','UPDATE', 'DELETE'] :\n",
    "            token_str = token.value.upper()\n",
    "            list_output.append(token_str)\n",
    "        \n",
    "        elif (last_token == 'DELETE' and token.value.upper() != 'FROM' and len(token.value) <= 3):\n",
    "            pass\n",
    "        \n",
    "        elif (last_token == 'UPDATE' and len(token.value) <= 3):\n",
    "            pass\n",
    "            \n",
    "        elif token.ttype is DDL and token.value.upper() == 'CREATE':\n",
    "            token_str = 'CREATE'\n",
    "            create_seen = True\n",
    "\n",
    "        elif token.is_keyword and token.value.upper() == 'RENAME':\n",
    "            rename_seen = True            \n",
    "  \n",
    "        elif token.is_keyword and token.value.upper() == 'TABLE' and rename_seen:\n",
    "            token_str = 'RENAME TABLE'\n",
    "            list_output.append(token_str)            \n",
    "            \n",
    "        elif token.ttype is DDL and token.value.upper() == 'DROP':\n",
    "            token_str = 'DROP'\n",
    "            drop_seen = True\n",
    "            list_output.append(token_str)\n",
    "\n",
    "        elif token.is_keyword and token.value.upper() == 'JOIN' and drop_seen:\n",
    "            token_str = 'DROP JOIN INDEX'\n",
    "            list_output[-1] = token_str\n",
    "        \n",
    "        elif token.is_keyword and token.value.upper() == 'INDEX' and drop_seen:\n",
    "            token_str = token_str + ' INDEX'\n",
    "            list_output[-1] = token_str\n",
    "            \n",
    "        elif token.is_keyword and token.value.upper() == 'INDEX' and create_seen:\n",
    "            token_str = token_str + ' INDEX'\n",
    "            list_output.append(token_str)\n",
    " \n",
    "        elif token.is_keyword and token.value.upper() == 'JOIN' and create_seen:\n",
    "            token_str = 'CREATE JOIN INDEX'\n",
    "            create_seen = False\n",
    "            list_output.append(token_str)\n",
    "\n",
    "        elif token.ttype is Name and token.value.upper() == 'SET' and create_seen:\n",
    "            token_str = 'CREATE SET TABLE'\n",
    "            create_seen = False\n",
    "            list_output.append(token_str)\n",
    "            \n",
    "        elif token.ttype is Name and token.value.upper() == 'MULTISET' and create_seen:\n",
    "            token_str = 'CREATE MULTISET TABLE'\n",
    "            create_seen = False\n",
    "            list_output.append(token_str)\n",
    "            \n",
    "        elif token.is_keyword and token.value.upper() == 'TABLE' and create_seen:\n",
    "            token_str = 'CREATE TABLE'\n",
    "            list_output.append(token_str)\n",
    "\n",
    "        elif token.is_keyword and token.value.upper() == 'VIEW' and create_seen:\n",
    "            token_str = 'CREATE VIEW'\n",
    "            list_output.append(token_str)\n",
    "        \n",
    "        elif token.ttype is DML and token.value.upper() == 'REPLACE':\n",
    "            replace_seen = True\n",
    "            \n",
    "        elif token.is_keyword and token.value.upper() == 'VIEW' and replace_seen:\n",
    "            token_str = 'REPLACE VIEW'\n",
    "            list_output.append(token_str)\n",
    "        \n",
    "        elif token.ttype is Name and token.value == 'HASH':\n",
    "            token_str = token_str + ' HASH '\n",
    "        \n",
    "        ## Aqui va agregando la info. de las tablas output Formato [DDL, ESQUEMA, TABLA]\n",
    "        elif token.ttype is Name and token.value not in ['HASH']:\n",
    "            list_output.append(token.value)\n",
    "            \n",
    "        \n",
    "        last_token = token.value.upper() \n",
    "                   \n",
    "    return 'DESCONOCIDO'    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10497,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_input_tables(folder, filename, code_lines, queryString, process_number):\n",
    "    '''\n",
    "    Función que procesa una query retornando su tabla output y la lista de tablas input\n",
    "    Si no existen tablas input, se coloca un mensaje que indica que no tiene condiciones\n",
    "    '''\n",
    "    \n",
    "    list_table_input  = []\n",
    "    list_table_output = []\n",
    "    columns = [ 'TIPO_ARCHIVO','ARCHIVO','LINEAS_DE_CODIGO','NUMERO_PASO','SENTENCIA_DML',\n",
    "               'ESQUEMA_OUTPUT','TABLA_OUTPUT','MULTINIVEL','ESQUEMA_INPUT','TABLA_INPUT',\n",
    "               'ALIAS_SUBQUERY', 'COLUMNA','VARIABLE','LOGICA_NEGOCIO', 'ALIAS_CAMPO']\n",
    "    \n",
    "    df_final_merge       = emptyDataframeExtractQuery(folder)\n",
    "    df_merge_table_input = emptyDataframeExtractQuery(folder)\n",
    "\n",
    "    list_syntax_large = ['CREATE MULTISET TABLE', 'DROP JOIN INDEX', 'CREATE INDEX']\n",
    "    \n",
    "    if (queryString):\n",
    "\n",
    "        parsed                  = sqlparse.parse(queryString)[0]\n",
    "        type_DML_DDL_principal  = parsed.token_first().value.upper()\n",
    "        list_table_output       = get_tables_output(queryString)\n",
    "        \n",
    "\n",
    "        if(list_table_output[0] not in list_syntax_large):\n",
    "                \n",
    "            if (type_DML_DDL_principal in ['DELETE', 'UPDATE']):\n",
    "\n",
    "                df_merge_table_input = get_query_tables_and_columns(queryString)\n",
    "                \n",
    "\n",
    "            elif (list_table_output[0] in ['DROP']):\n",
    "                output_table    = sqllib.get_query_tables(queryString)[0] \n",
    "                l_output_table  = output_table.split('.')\n",
    "                len_l_output_table = len(l_output_table)                \n",
    "                \n",
    "                # Para los casos que hay tablas volatiles sin esquema\n",
    "                if (len_l_output_table == 2):\n",
    "                    database    = l_output_table[0]\n",
    "                    table       = l_output_table[1]\n",
    "                    \n",
    "                    list_table_output.clear() \n",
    "                    list_table_output.append(type_DML_DDL_principal)\n",
    "                    list_table_output.append(database) \n",
    "                    list_table_output.append(table) \n",
    "                \n",
    "                elif (len_l_output_table == 1):\n",
    "                    table       = l_output_table[0]\n",
    "                    \n",
    "                    list_table_output.clear() \n",
    "                    list_table_output.append(type_DML_DDL_principal)\n",
    "                    list_table_output.append('N/A') \n",
    "                    list_table_output.append(table) \n",
    "                \n",
    "            else:\n",
    "                df_merge_table_input = get_query_tables_and_columns(queryString)\n",
    "\n",
    "    ## Si no tiene tablas, agregar info de no tener condicion de tablas input\n",
    "    #print(df_merge_table_input)\n",
    "    \n",
    "    if (df_merge_table_input.empty): \n",
    "        df_table_input = pd.DataFrame({\"MULTINIVEL\": ['0'],\n",
    "                                       \"ESQUEMA_INPUT\":[noConditionInput], \n",
    "                                       \"TABLA_INPUT\":[noConditionInput]})\n",
    "        \n",
    "        df_merge_table_input = df_merge_table_input.append(df_table_input, sort=True)\n",
    "        \n",
    "        \n",
    "\n",
    "    df_merge_table_input = df_merge_table_input.rename(columns={\n",
    "                                                    'NOMBRE_CAMPO'   : 'COLUMNA',\n",
    "                                                    'CAMPO_COMPUESTO': 'VARIABLE'})\n",
    "        \n",
    "    df_final_merge = df_final_merge.append(df_merge_table_input, sort=True)\n",
    "    \n",
    "\n",
    "    if (len(list_table_output) >= 3):\n",
    "        # Asigna los valores constantes a dicho dataframe\n",
    "        if(folder != 'Vistas'):\n",
    "            df_final_merge = df_final_merge.assign(TIPO_ARCHIVO = folder, \\\n",
    "                                                   ARCHIVO = filename, \\\n",
    "                                                   LINEAS_DE_CODIGO = code_lines, \\\n",
    "                                                   NUMERO_PASO = process_number, \\\n",
    "                                                   SENTENCIA_DML  = list_table_output[0], \\\n",
    "                                                   ESQUEMA_OUTPUT = list_table_output[1], \\\n",
    "                                                   TABLA_OUTPUT   = list_table_output[2])\n",
    "        else:\n",
    "            df_final_merge = df_final_merge.assign(TIPO_ARCHIVO = folder, \\\n",
    "                                                   ARCHIVO = filename, \\\n",
    "                                                   LINEAS_DE_CODIGO = code_lines, \\\n",
    "                                                   SENTENCIA_DML  = list_table_output[0], \\\n",
    "                                                   ESQUEMA_OUTPUT = list_table_output[1], \\\n",
    "                                                   TABLA_OUTPUT   = list_table_output[2])\n",
    "    \n",
    "    # Reordenando los campos del dataframe \n",
    "    df_final_merge = df_final_merge.replace(to_replace='nan.', value='', regex=True)\n",
    "    df_final_merge = df_final_merge.replace(to_replace='nan',  value='', regex=True)\n",
    "    df_final_merge = df_final_merge.reindex(columns , axis=1)\n",
    "    \n",
    "    return df_final_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10498,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_number_by_list(list_unique_str_num_query, last_number_subquery):\n",
    "    ''' \n",
    "    Función que crea el multinivel de las queries, \n",
    "    con secuencia: 1.1, 1.2, 1.2.1, según lista de multiveles ya creados\n",
    "    '''    \n",
    "    list_final = []\n",
    "    \n",
    "    lenght_list       = len(list_unique_str_num_query)\n",
    "    length_last_query = len(last_number_subquery)\n",
    "    \n",
    "    if (length_last_query == 1 and lenght_list > 1):        \n",
    "        for item in list_unique_str_num_query:\n",
    "            \n",
    "            if (len(item) == 3):\n",
    "                first_item = item.split('.')[0]\n",
    "                \n",
    "                if (first_item == last_number_subquery):\n",
    "                    list_final.append(item)\n",
    "                \n",
    "        if list_final:\n",
    "            last_multilevel      = max(list_final)\n",
    "            split                = last_multilevel[:-1]\n",
    "            last_number          = last_multilevel[-1]\n",
    "            next_number          = int(last_number) + 1\n",
    "            new_number_subquery  = split + str(next_number)\n",
    "        else:\n",
    "            new_number_subquery  = last_number_subquery + '.1'\n",
    "        \n",
    "    elif (length_last_query >= 3 and lenght_list > 1):      \n",
    "        for item in list_unique_str_num_query:\n",
    "            if (len(item) == length_last_query + 2 and \\\n",
    "               last_number_subquery in item):\n",
    "                list_final.append(item)\n",
    "        \n",
    "        if list_final:                \n",
    "            last_multilevel      = max(list_final)\n",
    "            split                = last_multilevel[:-1]\n",
    "            last_number          = last_multilevel[-1]\n",
    "            next_number          = int(last_number) + 1\n",
    "            new_number_subquery  = split + str(next_number)\n",
    "        else:\n",
    "            new_number_subquery  = last_number_subquery + '.1'\n",
    "                \n",
    "   \n",
    "        \n",
    "    else:\n",
    "        new_number_subquery  = '1.1'   \n",
    "        \n",
    "        \n",
    "    return new_number_subquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10499,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generateNumberedListSubquery(flag_get_in_subquery, flag_lastjoin_is_subquery,\n",
    "                                 flag_union_join_query,count_query_multilevel, \n",
    "                                 last_number_subquery, list_unique_str_num_query):\n",
    "    \n",
    "    ''' \n",
    "    Función que crea el multinivel de las queries, con secuencia: 1.1, 1.2, 1.2.1, etc\n",
    "    '''\n",
    "    \n",
    "    point = '.'\n",
    "    new_number_subquery = None\n",
    "    entered_in_other_rule = False\n",
    "    length_last_query = len(last_number_subquery)\n",
    "    \n",
    "    '''\n",
    "    print(\"last_number_subquery      ---> \"+ last_number_subquery + \", length_last_query ---> \"+ str(length_last_query))\n",
    "    print(\"count_query_multilevel    ---> \"+ str(count_query_multilevel))\n",
    "    print(\"flag_get_in_subquery      ---> \"+ str(flag_get_in_subquery))\n",
    "    print(\"flag_lastjoin_is_subquery ---> \"+ str(flag_lastjoin_is_subquery))\n",
    "    print(\"flag_union_join_query     ---> \"+ str(flag_union_join_query))\n",
    "    '''\n",
    "    \n",
    "    '''Sección para los casos de UNION'''\n",
    "    if (flag_union_join_query and flag_get_in_subquery == False \\\n",
    "    and flag_lastjoin_is_subquery == False):\n",
    "        ## Si es = 1 entonces ponerle .1 , sino es sumarle 1 al ultimo valor        \n",
    "        if (length_last_query == 1):\n",
    "            new_number_subquery = str(int(last_number_subquery) + 1)\n",
    "        else:\n",
    "            split               = last_number_subquery[:-1]\n",
    "            next_number         = int(last_number_subquery[-1]) + 1\n",
    "            new_number_subquery = split + str(next_number)\n",
    "\n",
    "            \n",
    "    elif (flag_union_join_query == False):\n",
    "        \n",
    "        '''Sección donde entra a las subqueries'''\n",
    "        ## Abre parentesis y consigue la primera subquery\n",
    "        if (flag_lastjoin_is_subquery == False):\n",
    "\n",
    "            if (length_last_query in [1,2] and flag_get_in_subquery):\n",
    "                entered_in_other_rule = True\n",
    "                new_number_subquery = generate_number_by_list(list_unique_str_num_query, last_number_subquery)\n",
    "\n",
    "            elif (length_last_query >= 3 and flag_get_in_subquery):\n",
    "                split               = last_number_subquery[:-1]\n",
    "                next_number         = int(last_number_subquery[-1]) + 1\n",
    "                new_number_subquery = split + str(next_number)\n",
    "\n",
    "            elif (length_last_query == count_query_multilevel):\n",
    "                new_number_subquery = str(last_number_subquery) + '.1'\n",
    "\n",
    "        '''Sección donde sale de las subqueries'''     \n",
    "        if (length_last_query >= 3 and flag_get_in_subquery == False):\n",
    "            # Se elimina el ultimo numero del multinivel para volver al principal del mismo\n",
    "            number_subquery_list = last_number_subquery.split('.')\n",
    "            number_subquery_list = number_subquery_list[:-1]\n",
    "            \n",
    "            for item in number_subquery_list:\n",
    "                if new_number_subquery == None:\n",
    "                    new_number_subquery = item\n",
    "                else:\n",
    "                    new_number_subquery = new_number_subquery + '.' + item\n",
    "        \n",
    "            entered_in_other_rule = True\n",
    "\n",
    "        elif (flag_lastjoin_is_subquery):\n",
    "            new_number_subquery = generate_number_by_list(list_unique_str_num_query, last_number_subquery)\n",
    "        \n",
    "\n",
    "        if (length_last_query == count_query_multilevel and entered_in_other_rule == False):\n",
    "            new_number_subquery = str(last_number_subquery) + '.1'   \n",
    "            \n",
    "\n",
    "    return new_number_subquery\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_tables_and_columns(query):\n",
    "    \"\"\"\n",
    "    Función que obtiene todos los objetos de una query(tablas, campos, reglas) \n",
    "    \n",
    "    Descripción: Por cada nivel de subquery que consiga, inserta en un dataframe los \n",
    "    objetos que consiga según ese nivel.\n",
    "    \"\"\"    \n",
    "    \n",
    "    '''LISTAS Y MANEJO DE OBJETOS (TABLAS, CAMPOS Y REGLAS)'''\n",
    "    \n",
    "    \n",
    "    column                    = None # Contiene el último valor de un campo del ´SELECT´\n",
    "    \n",
    "    str_rule_agg              = '' # Cadena que va agregando la regla a medida que avanza el token\n",
    "    \n",
    "    tables                    = [] # Lista interna de tabla con formato [ESQUEMA, TABLA, ALIAS]\n",
    "    \n",
    "    column_list               = [] # Lista de columnas con formato [ESQUEMA.CAMPO1, ALIAS_1]\n",
    "    \n",
    "    column_list_internal      = [] # Lista de columnas con formato [ESQUEMA.CAMPO1, ALIAS_1]\n",
    "    \n",
    "    column_list_total         = [] # Lista total de todas las columnas con formato [ESQUEMA.CAMPO1, ALIAS_1]  \n",
    "                                   # Ej: [[ESQUEMA.CAMPO1, ALIAS_1],[ESQUEMA.CAMPO2, ALIAS_2] ]\n",
    "    \n",
    "    table_list                = [] # Lista interna de tabla con formato [ESQUEMA, TABLA, ALIAS] \n",
    "    \n",
    "    table_list_total          = [] # Lista total de todas las tablas encontradas en una query o subquery.     \n",
    "                                   # Ej: [[ESQUEMA1, TABLA1, ALIAS1],[ESQUEMA1, TABLA2, ALIAS2] ]\n",
    "                                   \n",
    "    rule_select_list          = [] # Lista interna de una regla o función encontrada en un SELECT.\n",
    "                                   # Con formato [str_rule_agg, column_alias]\n",
    "                                   # Ej: [MAX(COLUMN), ALIAS]\n",
    "    \n",
    "    rule_select_list_total    = [] # Lista total de todas las reglas y funciones encontradas en un SELECT\n",
    "                                   # Con formato [[str_rule_agg, column_alias],[str_rule_agg, column_alias]]\n",
    "                                   # Ej: [[MAX(CAMPO_1), ALIAS_1], [SUBSTR(CAMPO_2), ALIAS_2]]\n",
    "    \n",
    "    rule_generic_list_total   = [] # Lista total de todas las reglas encontradas en un ´JOIN´ o ´WHERE´\n",
    "    \n",
    "    \n",
    "    '''FLAGS PARA EL CONTROL DE LAS REGLAS INTERNAS'''\n",
    "    flag_rule_select           = False  # Se activa si consigue una funcion como regla [functions_sql]\n",
    "    flag_rule_filter           = False  # Se activa si consigue un ´ON´ o ´WHERE´\n",
    "    flag_end_rule_as           = False  # Se activa si es el final de una regla con un ALIAS cuando consigue ´AS´\n",
    "    flag_rule_case             = False  # Se activa si es el final de una regla de CASE cuando consigue ´END´ o ´AS´\n",
    "    flag_rule_case_internal    = False  # Se activa si es una regla de CASE dentro de otra función\n",
    "    flag_rule_in_before_select = False  # Se activa si consigue '(' después de un 'IN'\n",
    "        \n",
    "    flag_open_parentheses_rule = False  # Se activa cuando estoy dentro de una regla y consigue un parantesis ´(´\n",
    "    flag_count_parentheses_rule = False # Se activa cuando los parentesis abiertos y cerrados son iguales en una regla\n",
    "    \n",
    "    \n",
    "    '''MANEJO DEL TOKEN DE QUERY'''\n",
    "    last_keyword     = None     # Contiene el último valor del Keyword (palabra reservada SQL) del token \n",
    "    last_token       = None     # Contiene el último valor del token\n",
    "    subquery_alias   = None     # Contiene el ALIAS de una subquery\n",
    "    column_alias     = None     # Contiene el ALIAS de una subquery\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''VARIABLES PARA EXTRAER LAS TABLAS INPUT'''\n",
    "    dml_ddl_name      = None\n",
    "    not_get_from      = False\n",
    "    from_clausule     = False\n",
    "    flag_create_table = False\n",
    "    \n",
    "        \n",
    "    '''DATAFRAMES'''    \n",
    "    df_final_tables             = emptyDataframeTablesQuery(None)\n",
    "    df_internal_table           = emptyDataframeTablesQuery(None)\n",
    "    \n",
    "    df_final_columns            = emptyDataframeColumnsQuery(None)\n",
    "    df_internal_column          = emptyDataframeColumnsQuery(None)\n",
    "    \n",
    "    df_merge_table_column_input = emptyDataframeTablesColumnsInputQuery(None)\n",
    "    \n",
    "    \n",
    "    '''MULTINIVEL DE SUBQUERIES'''\n",
    "    flag_open_parentheses_subq  = False  # Se activa cuando se abre parentesis por una subquery\n",
    "    flag_close_parentheses_subq = False  # Se activa cuando se cierra parentesis por una subquery\n",
    "    flag_is_subquery            = False  # Se activa cuando consigue una subquery\n",
    "    flag_lastjoin_is_subquery   = False  # Se activa cuando hay una subquery dentro de otra subquery\n",
    "    num_query                   = 0      # Nivel del select o subquery.\n",
    "    \n",
    "    '''CONTADORES DE PARENTESIS PARA SABER CUANDO EMPIEZA O TERMINA UNA QUERY'''\n",
    "    count_query_multilevel     = 0  \n",
    "    count_open_parentheses     = 0        # Contador de parentesis '(' externos \n",
    "    count_close_parentheses    = 0        # Contador de parentesis ')' externos \n",
    "    count_open_parentheses_rule  = 0      # Contador de parentesis '(' de reglas \n",
    "    count_close_parentheses_rule = 0      # Contador de parentesis ')' de reglas\n",
    "    count_open_parentheses_internal  = 0  # Contador de parentesis '(' internos de una subquery\n",
    "    count_close_parentheses_internal = 0  # Contador de parentesis ')' internos de una subquery\n",
    "    \n",
    "    \n",
    "    list_unique_str_num_query  = set({'1'})\n",
    "    str_num_query              = '1'\n",
    "    last_number_subquery       = '1'\n",
    "    \n",
    "    \n",
    "    cast_data_type_list = ['INTEGER', 'CHAR', 'DECIMAL', 'VARCHAR','TIMESTAMP']\n",
    "        \n",
    "    stop_from_list = ['AS','CASE','WHEN','ON','AND','END','WHERE','GROUP','OVER','PARTITION','SET']\n",
    "    \n",
    "    functions_ignored = ['COUNT', 'MIN', 'MAX', 'SUM', 'FROM_UNIXTIME', 'DEC', 'QUALIFY',\n",
    "    'CAST', 'CONVERT', 'ZEROIFNULL','SUBSTR','SUBSTRING','ROW_NUMBER', 'RANK', 'ADD_MONTHS',\n",
    "    'COALESCE', 'CHAR', 'INTEGER', 'TRIM', 'OVER', 'FORMAT', 'DATE_FORMAT','CHARACTER_LENGTH',\n",
    "    'CHAR_LENGTH']\n",
    "    \n",
    "    dates_list = ['SECOND', 'MINUTE', 'HOUR', 'DAY', 'YEAR','MONTH','BOTH', 'TRAILING']\n",
    "    \n",
    "    columns_syntax_keywords = ['CURRENT_TIMESTAMP','CURRENT_TIME']\n",
    "    \n",
    "    # Lista de palabras reservadas que DEBEN ser consideras al obtener una regla.\n",
    "    keywords_ignored = ['AS', 'AND', 'OR', 'IN', 'IS', 'NOT','NOT NULL', 'NULL',\n",
    "    'LIKE', 'CASE', 'WHEN', 'ON', 'CURRENT_DATE']\n",
    "\n",
    "    # Lista de funciones reservadas de SQL ANSI\n",
    "    functions_sql = ['AVG', 'TITLE', 'OVER', 'OREPLACE','COUNT', 'MIN', 'MAX', 'SUM', \n",
    "    'FROM_UNIXTIME', 'DEC', 'CAST', 'CONVERT','ZEROIFNULL','SUBSTR','ROW_NUMBER','RANK',\n",
    "    'COALESCE', 'CHAR', 'TRIM', 'FORMAT', 'EXTRACT','ADD_MONTHS','DISTINCT', 'QUALIFY', \n",
    "    'DATE_FORMAT', 'ELSE', 'LAST_DAY', 'SUBSTRING', 'UPPER', 'TRUNC','CHARACTER_LENGTH'\n",
    "    ]\n",
    "    # quité el INTEGER de aqui\n",
    "    \n",
    "    # Lista de funciones reservadas de SQL ANSI para saber que comienza una regla\n",
    "    functions_rules = ['ON',  'WHERE']\n",
    "    \n",
    "    # Lista de funciones reservadas de SQL ANSI para saber que lo siguiente será un campo.\n",
    "    functions_before_column = ['SEL', 'SELECT', 'ON', 'END', 'THEN', 'DISTINCT', 'WHERE']\n",
    "    \n",
    "    # Lista de funciones reservadas de SQL ANSI para las uniones de tablas\n",
    "    table_syntax_joins = ['FROM', 'JOIN', 'INNER JOIN', 'LEFT JOIN', 'LEFT OUTER JOIN', \n",
    "\t'RIGHT JOIN', 'RIGHT OUTER JOIN','CROSS JOIN']\n",
    "    \n",
    "    union_joins_list = ['UNION', 'UNION ALL']\n",
    "        \n",
    "   \n",
    "    # Lista de sintaxis reservadas de SQL.     \n",
    "    table_syntax_keywords = [\n",
    "        # SELECT queries\n",
    "        'FROM', 'WHERE', 'JOIN', 'INNER JOIN', 'LEFT JOIN', 'LEFT OUTER JOIN', \n",
    "\t\t'RIGHT JOIN', 'RIGHT OUTER JOIN', 'ON', 'UNION', 'UNION ALL','CROSS JOIN',\n",
    "        # INSERT queries\n",
    "        'INTO', 'VALUES',\n",
    "        # UPDATE queries\n",
    "        'UPDATE', 'SET',\n",
    "        # Hive queries\n",
    "        'TABLE',  # INSERT TABLE\n",
    "    ]\n",
    "\n",
    "    for token in sqllib.get_query_tokens(query):\n",
    "            \n",
    "        \n",
    "        ## Homologar en caso que se consiga un 'SEL' en vez de 'SELECT'\n",
    "        if ((token.ttype is Name and token.value.upper() == 'SEL')):\n",
    "            token.ttype = DML\n",
    "            token.value = 'SELECT' \n",
    "        \n",
    "        if (count_open_parentheses > 0):\n",
    "            flag_is_subquery = True\n",
    "        \n",
    "\n",
    "        \n",
    "        ## Es una regla de IN con Subquery y no debe cortar el proceso.\n",
    "        if (token.value == '('):\n",
    "            if (last_token in ['IN', 'EXISTS', 'NOT EXISTS']):\n",
    "                flag_rule_in_before_select = True\n",
    "                    \n",
    "        \n",
    "        ## Cálculo de multiniveles de queries \n",
    "        if (flag_is_subquery):\n",
    "            if (token.value == '('):\n",
    "                count_open_parentheses += 1\n",
    "                    \n",
    "            elif (token.value == ')'):\n",
    "                count_close_parentheses += 1\n",
    "        \n",
    "        \n",
    "        ## Verifica si despues del IN viene un SELECT\n",
    "        if (flag_rule_in_before_select):\n",
    "            if(token.value == 'SELECT' and last_token == '('):\n",
    "                flag_rule_in_before_select = True\n",
    "            else:\n",
    "                flag_rule_in_before_select = False\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Una vez se consiga el fin de la subquery o UNION de querys, inserta en un df sus caracteristicas\n",
    "        if (token.ttype is Name and last_token in [')', 'AS']): \n",
    "            \n",
    "            ## Si los parentesis externos son iguales, es porque se cerró la subquery grande\n",
    "            if (count_open_parentheses == count_close_parentheses and \\\n",
    "                count_open_parentheses > 0):\n",
    "                \n",
    "                \n",
    "                # Obtiene el alias de la subquery\n",
    "                subquery_alias = token.value\n",
    " \n",
    "                # Obtiene el ultimo filtro que esté acumulado de la subquery\n",
    "                if (flag_rule_filter or flag_rule_select):\n",
    "                    flag_rule_filter  = False\n",
    "                    flag_rule_select  = False\n",
    "                    str_rule_agg = str_rule_agg.replace(\") \"+ last_token, '')\n",
    "                    rule_generic_list_total.append(str_rule_agg)\n",
    "                    str_rule_agg = ''\n",
    "                \n",
    "                \n",
    "                df_internal_table = insertDataframeTablesQuery(\n",
    "                                    df_internal_table, str_num_query,\n",
    "                                    table_list_total,subquery_alias ) \n",
    "                \n",
    "                df_final_tables   = df_final_tables.append(df_internal_table, ignore_index = True) \n",
    "                \n",
    "                \n",
    "                # Acumulación de objetos de campos y reglas internas\n",
    "                df_internal_column = insertDataframeColumnsQuery(\n",
    "                                    df_internal_column, str_num_query,\n",
    "                                    column_list, column_list_total, rule_select_list_total,\n",
    "                                    rule_generic_list_total)            \n",
    "                \n",
    "                df_final_columns = df_final_columns.append(df_internal_column, ignore_index = True) \n",
    "\n",
    "    \n",
    "                ## limpia los objetos en cada nivel de subselect\n",
    "                df_internal_table            = emptyDataframeTablesQuery(None)\n",
    "                df_internal_column           = emptyDataframeColumnsQuery(None)\n",
    "                column_list_total            = []                \n",
    "                column_list_internal         = []\n",
    "                table_list_total             = []\n",
    "                column_list                  = []\n",
    "                table_list                   = []\n",
    "                rule_select_list_total       = []\n",
    "                rule_generic_list_total      = []\n",
    "                column_alias = None\n",
    "                \n",
    "        \n",
    "                \n",
    "                '''   \n",
    "                print(\"count_open_parentheses  = \"+ str(count_open_parentheses))\n",
    "                print(\"count_close_parentheses = \"+ str(count_close_parentheses))\n",
    "                print(\"count_open_parentheses_internal  = \" + str(count_open_parentheses_internal))\n",
    "                print(\"count_close_parentheses_internal = \" + str(count_close_parentheses_internal))\n",
    "                print(\"flag_open_parentheses_subq = \" + str(flag_open_parentheses_subq))\n",
    "                \n",
    "                ''' \n",
    "                \n",
    "                flag_is_subquery            = False \n",
    "                not_get_from                = False\n",
    "                flag_rule_in                = False                \n",
    "                flag_rule_case              = False\n",
    "                flag_rule_case_internal     = False\n",
    "                flag_open_parentheses_subq  = False   \n",
    "                flag_close_parentheses_subq = True\n",
    "                count_open_parentheses      = 0\n",
    "                count_close_parentheses     = 0  \n",
    "                count_open_parentheses_rule  = 0\n",
    "                count_close_parentheses_rule = 0\n",
    "                \n",
    "                    \n",
    "                str_num_query = generateNumberedListSubquery(False,\n",
    "                                                             flag_lastjoin_is_subquery,\n",
    "                                                             False,\n",
    "                                                             count_query_multilevel,\n",
    "                                                             str_num_query,\n",
    "                                                             list_unique_str_num_query) \n",
    "                \n",
    "                if (str_num_query == None):\n",
    "                    str_num_query = '1'\n",
    "                    \n",
    "                last_number_subquery  =   str_num_query \n",
    "                list_unique_str_num_query.add(str_num_query)\n",
    "                \n",
    "            \n",
    "                \n",
    "        ## Cálculo de multiniveles de queries internas (subquery dentro de subquery)\n",
    "        if (flag_lastjoin_is_subquery and len(str_num_query) > 3 and flag_open_parentheses_subq):\n",
    "            \n",
    "            if (token.value == '('):\n",
    "                count_open_parentheses_internal += 1\n",
    "            elif (token.value == ')'):\n",
    "                count_close_parentheses_internal += 1\n",
    "         \n",
    "            if (token.ttype is Name and last_token == ')'):   \n",
    "                if ((count_open_parentheses_internal == count_close_parentheses_internal and \\\n",
    "                    count_open_parentheses_internal > 0 and flag_open_parentheses_subq) or \\\n",
    "                    (count_open_parentheses-1 == count_close_parentheses)):\n",
    "                                       \n",
    "                    # Obtiene el ultimo filtro que esté acumulado de la subquery\n",
    "                    if (flag_rule_filter and last_keyword != 'ROW_NUMBER'):\n",
    "                        flag_rule_filter = False\n",
    "                        str_rule_agg = str_rule_agg.replace(\") \"+ last_token, '')\n",
    "                        rule_generic_list_total.append(str_rule_agg)\n",
    "                        str_rule_agg = ''\n",
    "                    \n",
    "                    \n",
    "                    # Acumulación de objetos de tablas\n",
    "                    df_internal_table = insertDataframeTablesQuery(\n",
    "                                        df_internal_table, str_num_query,\n",
    "                                        table_list_total,subquery_alias ) \n",
    "\n",
    "                    df_final_tables   = df_final_tables.append(df_internal_table, ignore_index = True) \n",
    "\n",
    "\n",
    "                    # Acumulación de objetos de campos y reglas internas\n",
    "                    df_internal_column = insertDataframeColumnsQuery(\n",
    "                                        df_internal_column, str_num_query,\n",
    "                                        column_list, column_list_total, rule_select_list_total,\n",
    "                                        rule_generic_list_total)            \n",
    "\n",
    "                    df_final_columns = df_final_columns.append(df_internal_column, ignore_index = True) \n",
    "\n",
    "\n",
    "                    ## limpia los objetos en cada nivel de subselect\n",
    "                    df_internal_table            = emptyDataframeTablesQuery(None)\n",
    "                    df_internal_column           = emptyDataframeColumnsQuery(None)\n",
    "                    column_list_total            = []\n",
    "                    column_list_internal         = []\n",
    "                    table_list_total             = []\n",
    "                    column_list                  = []\n",
    "                    table_list                   = []\n",
    "                    rule_select_list_total       = []\n",
    "                    rule_generic_list_total      = []\n",
    "                    flag_rule_filter             = False              \n",
    "                    flag_rule_case               = False\n",
    "                    flag_rule_case_internal      = False\n",
    "                    not_get_from                 = False\n",
    "                    \n",
    "                    \n",
    "                    '''\n",
    "                    print(\"count_open_parentheses  = \"+ str(count_open_parentheses))\n",
    "                    print(\"count_close_parentheses = \"+ str(count_close_parentheses))\n",
    "                    print(\"count_open_parentheses_internal  = \" + str(count_open_parentheses_internal))\n",
    "                    print(\"count_close_parentheses_internal = \" + str(count_close_parentheses_internal))\n",
    "                    print(\"flag_open_parentheses_subq = \" + str(flag_open_parentheses_subq))\n",
    "                    '''\n",
    "                    \n",
    "                    count_open_parentheses_internal  = 0\n",
    "                    count_close_parentheses_internal = 0  \n",
    "                    count_open_parentheses_rule  = 0\n",
    "                    count_close_parentheses_rule = 0\n",
    "                    flag_close_parentheses_subq      = True\n",
    "\n",
    "                    \n",
    "                    str_num_query = generateNumberedListSubquery(False,\n",
    "                                                                 flag_lastjoin_is_subquery,\n",
    "                                                                 False,\n",
    "                                                                 count_query_multilevel,\n",
    "                                                                 str_num_query,\n",
    "                                                                 list_unique_str_num_query) \n",
    "\n",
    "                    \n",
    "                    last_number_subquery  =   str_num_query \n",
    "                    list_unique_str_num_query.add(str_num_query)\n",
    "  \n",
    "            \n",
    "        ## Se insertan los ultimos campos y tablas encontrados, antes de un subselect o fin de la query.\n",
    "        if ((token.ttype is DML and token.value.upper() == 'SELECT'\\\n",
    "          or token.ttype is Punctuation and token.value == ';') and \\\n",
    "            flag_rule_in_before_select == False and \\\n",
    "            flag_rule_select == False and \\\n",
    "           (last_token not in functions_rules or last_keyword not in functions_rules)):       \n",
    "                \n",
    "            if (num_query > 0):  \n",
    "                \n",
    "                # Para los casos que vengan consultas con UNION\n",
    "                if (last_token in union_joins_list):\n",
    "                    last_number_subquery = str_num_query\n",
    "                    str_num_query = generateNumberedListSubquery(False,\n",
    "                                                                 False,\n",
    "                                                                 True,\n",
    "                                                                 count_query_multilevel,\n",
    "                                                                 str_num_query,\n",
    "                                                                 list_unique_str_num_query) \n",
    "                \n",
    "                # Calcular el multinivel siempre que entra a la subquery\n",
    "                if (last_token == '(' and last_keyword in table_syntax_joins):\n",
    "                                    \n",
    "                    flag_is_subquery = True\n",
    "                    flag_open_parentheses_subq = True\n",
    "                        \n",
    "                        \n",
    "                    if(len(str_num_query) >= 3):\n",
    "                        flag_lastjoin_is_subquery = True\n",
    "                    \n",
    "                    if(count_open_parentheses == 0):\n",
    "                        count_open_parentheses += 1\n",
    "                        \n",
    "                    count_query_multilevel += 1\n",
    "                    \n",
    "                    ## Comienza contador de parentesis interno de la subquery\n",
    "                    if (flag_lastjoin_is_subquery and len(str_num_query) > 1):\n",
    "                        \n",
    "                        ## Query de más afuera donde sería la principal y acumula esos parentesis\n",
    "                        if (len(str_num_query) > 3):\n",
    "                            count_open_parentheses_internal  = 0\n",
    "                            count_close_parentheses_internal = 0 \n",
    "                        \n",
    "                        count_open_parentheses_internal += 1\n",
    "                    \n",
    "                    last_number_subquery  =   str_num_query   \n",
    "                    '''\n",
    "                    print(\"EL ESTADO DE LOS PARENTESIS ES = \")\n",
    "                    print(\"count_open_parentheses  = \"+ str(count_open_parentheses))\n",
    "                    print(\"count_close_parentheses = \"+ str(count_close_parentheses))\n",
    "                    print(\"ANTES DE ENTRAR generateNumberedListSubquery 3\")\n",
    "                    '''\n",
    "                    str_num_query = generateNumberedListSubquery(True,\n",
    "                                                                 flag_lastjoin_is_subquery,\n",
    "                                                                 False,\n",
    "                                                                 count_query_multilevel,\n",
    "                                                                 str_num_query,\n",
    "                                                                 list_unique_str_num_query) \n",
    "                   \n",
    "                    \n",
    "                    list_unique_str_num_query.add(str_num_query)\n",
    "                    \n",
    "            if (last_token not in functions_rules):\n",
    "                last_keyword = 'SELECT'  \n",
    "           \n",
    "                \n",
    "            num_query += 1            \n",
    "            #Omite el 'SELECT' principal\n",
    "            if (num_query >= 2):                \n",
    " \n",
    "                # Obtiene el ultimo filtro que esté acumulado de la subquery\n",
    "                if (token.ttype is Punctuation and token.value == ';' and flag_rule_filter):\n",
    "                    flag_rule_filter  = False\n",
    "                    flag_rule_select  = False\n",
    "                    rule_generic_list_total.append(str_rule_agg)\n",
    "                    str_rule_agg = ''\n",
    "                    \n",
    "                    \n",
    "                # Acumulación de objetos de tablas\n",
    "                df_internal_table = insertDataframeTablesQuery(\n",
    "                                    df_internal_table, last_number_subquery,\n",
    "                                    table_list_total,subquery_alias ) \n",
    "                \n",
    "                df_final_tables   = df_final_tables.append(df_internal_table, ignore_index = True) \n",
    "                \n",
    "                \n",
    "                # Acumulación de objetos de campos y reglas internas\n",
    "                df_internal_column = insertDataframeColumnsQuery(\n",
    "                                    df_internal_column, last_number_subquery,\n",
    "                                    column_list, column_list_total, rule_select_list_total,\n",
    "                                    rule_generic_list_total)            \n",
    "                \n",
    "                df_final_columns = df_final_columns.append(df_internal_column, ignore_index = True) \n",
    "\n",
    "    \n",
    "                ## limpia los objetos en cada nivel de subselect\n",
    "                df_internal_table            = emptyDataframeTablesQuery(None)\n",
    "                df_internal_column           = emptyDataframeColumnsQuery(None)\n",
    "                column_list_total            = []                \n",
    "                table_list_total             = []\n",
    "                column_list_internal         = []\n",
    "                column_list                  = []\n",
    "                table_list                   = []\n",
    "                rule_select_list_total       = []\n",
    "                rule_generic_list_total      = []\n",
    "                str_rule_agg = ''\n",
    "                flag_is_subquery             = True\n",
    "                flag_rule_filter             = False\n",
    "                not_get_from                 = False\n",
    "    \n",
    "        #print(['1', '***  ' + str(str_num_query)  + '  ***', token, token.ttype, last_token, last_keyword])              \n",
    "        \n",
    "    \n",
    "    ####################### EXTRACCIÓN DE LOGICAS DE CODIGO ########################################\n",
    "        '''\n",
    "        Sección que recupera las reglas internas y los filtros de una query,\n",
    "        junto con sus alias respectivos\n",
    "        Retorna una lista con el formato: [str_rule_agg, column_alias]\n",
    "        A su vez si inserta en una lista        \n",
    "        ''' \n",
    "\n",
    "        '''\n",
    "        print(\"flag_is_subquery              = \" + str(flag_is_subquery))\n",
    "        print(\"not_get_from                  = \" + str(not_get_from))\n",
    "        print(\"flag_rule_filter              = \" + str(flag_rule_filter))\n",
    "        print(\"flag_rule_select              = \" + str(flag_rule_select))\n",
    "        print(\"flag_end_rule_as              = \" + str(flag_end_rule_as))\n",
    "        print(\"flag_rule_case                = \" + str(flag_rule_case))\n",
    "        print(\"flag_rule_case_internal       = \" + str(flag_rule_case_internal)) \n",
    "        print(\"flag_rule_in_before_select    = \" + str(flag_rule_in_before_select))       \n",
    "        print(\"count_open_parentheses        = \" + str(count_open_parentheses))\n",
    "        print(\"count_close_parentheses       = \" + str(count_close_parentheses))\n",
    "        print(\"flag_open_parentheses_rule    = \" + str(flag_open_parentheses_rule)+ \"\\n\" )\n",
    "        print(\"count_open_parentheses_rule   = \" + str(count_open_parentheses_rule))\n",
    "        print(\"count_close_parentheses_rule  = \" + str(count_close_parentheses_rule)) \n",
    "        print(\"flag_count_parentheses_rule   = \" + str(flag_count_parentheses_rule))             \n",
    "        '''\n",
    "        \n",
    "        # Para los casos que toma el subquery_alias como column_alias y continua con algun JOIN\n",
    "        token_value_clean = \" \".join(token.value.upper().split())        \n",
    "        if (token_value_clean in table_syntax_joins and \\\n",
    "            column_alias == last_token):\n",
    "            #print(\"Quitar el alias de la regla = \"+ column_alias)\n",
    "            column_alias = None\n",
    "            \n",
    "        \n",
    "\n",
    "        ##CASE, ON, FUNCTIONS_SQL   \n",
    "        if ((token.value.upper() in keywords_ignored or token.value.upper() in functions_sql) \\\n",
    "            and flag_rule_filter == False \n",
    "            and flag_rule_select == False):\n",
    "            \n",
    "            flag_rule_case_internal = False            \n",
    "            \n",
    "            if (token.value.upper() in ['CASE']): \n",
    "                flag_rule_case   = True\n",
    "                flag_rule_select = True                \n",
    "                                               \n",
    "            elif (token.value.upper() in ['ON','QUALIFY']):        \n",
    "                flag_rule_filter = True  \n",
    "                \n",
    "            elif (token.value.upper() in functions_sql and flag_rule_filter == False):          \n",
    "                flag_rule_select = True  \n",
    "                \n",
    "            elif (token.value.upper() == 'NULL'and (last_token == ',' or last_keyword == 'SELECT')):        \n",
    "                flag_rule_select = True  \n",
    " \n",
    "        elif (flag_rule_select and token.value.upper() in ['CASE']):  \n",
    "            flag_rule_case_internal   = True \n",
    "            \n",
    "            \n",
    "        elif (token.value.upper() in ['CASE']):  \n",
    "            flag_rule_case   = True \n",
    "        \n",
    "        \n",
    "        ## Casos de campos constantes. Ejemplo: 'Clientes Con Campana' (Varchar(50)) Descripcion \n",
    "        elif ((token.ttype is Literal.String.Single or \\\n",
    "               token.ttype is Literal.Number.Integer)\n",
    "              and flag_rule_filter == False  \\\n",
    "              and flag_rule_select == False  \\\n",
    "              and (last_token == ',' or last_keyword == 'SELECT') ):    \n",
    "            flag_rule_select = True  \n",
    "            \n",
    "        \n",
    "        \n",
    "        # Para concluir los casos de CAST ('' AS CHAR(40)) AS ALIAS_CAMPO\n",
    "        # Y cualquier otro ALIAS de campos en general\n",
    "        elif (token.is_keyword and token.value.upper() == 'AS' \\\n",
    "            and last_token in [')', 'END',\"'\"+'0000'+\"'\"] ):\n",
    "        \n",
    "            if (count_open_parentheses_rule == 0):\n",
    "                flag_end_rule_as = True\n",
    "            else:\n",
    "                flag_end_rule_as = False\n",
    "            \n",
    "            if (last_token == 'END' and last_keyword == 'END' and count_open_parentheses_rule == 0):\n",
    "                flag_rule_select = False                  \n",
    "                flag_rule_case   = False\n",
    "        \n",
    "           \n",
    "        ## No interrumpir la acumulacion de logica asi consiga ',' \n",
    "        ## Si no hay conseguido AS, si no es un filtro, si hay un parentesis abiertos '('\n",
    "        elif ((token.ttype is Punctuation and token.value == ',' or \\\n",
    "              token.ttype is Name and token.value.upper() not in functions_sql) \\\n",
    "            and str_rule_agg != ''  \\\n",
    "            and flag_end_rule_as == False \\\n",
    "            and flag_rule_filter == False \\\n",
    "            and flag_open_parentheses_rule == False):\n",
    "            \n",
    "\n",
    "            if (flag_rule_case and last_keyword == 'END' \\\n",
    "            and flag_rule_case_internal == False):\n",
    "                flag_rule_case = False\n",
    "            \n",
    "            \n",
    "            # Para los casos por ejemplo: MAX(CASE WHEN RNK=1 THEN XCTA END) NTRJ1\n",
    "            if (flag_rule_case_internal and last_keyword == 'END' \\\n",
    "            and count_open_parentheses_rule == 0):\n",
    "                flag_rule_case_internal = False\n",
    "            \n",
    "            \n",
    "            ## Fin de reglas en el SELECT\n",
    "            if (flag_rule_case == False and flag_rule_case_internal == False\n",
    "               and count_open_parentheses_rule == 0 ):\n",
    "                \n",
    "                if(token.ttype is Name and last_token in [')','AS'] \\\n",
    "                and last_keyword not in ['ANY','BY','DESC']):\n",
    "                    column_alias = token.value.upper()\n",
    "                \n",
    "                flag_rule_select = False \n",
    "                rule_select_list = [str_rule_agg, column_alias]\n",
    "                rule_select_list_total.append(rule_select_list)\n",
    "                str_rule_agg = ''                \n",
    "                count_open_parentheses_rule  = 0\n",
    "                count_close_parentheses_rule = 0\n",
    "                flag_count_parentheses_rule = False\n",
    "                last_keyword = 'SELECT'\n",
    "         \n",
    "       ## Fin del ON o WHERE         \n",
    "        elif ((flag_rule_filter and token.value.upper() in table_syntax_keywords \\\n",
    "              and token.value.upper() != 'FROM' and flag_rule_in_before_select == False)\\\n",
    "              or (token.ttype is Punctuation and token.value in [';'])):\n",
    "            \n",
    "                flag_rule_filter = False\n",
    "                str_rule_agg = str_rule_agg.replace(\") \"+ last_token, '')\n",
    "                rule_generic_list_total.append(str_rule_agg)\n",
    "                str_rule_agg = ''\n",
    "                count_open_parentheses_rule  = 0\n",
    "                count_close_parentheses_rule = 0\n",
    "                flag_count_parentheses_rule = False\n",
    "                \n",
    "        elif token.ttype is Name:\n",
    "            \n",
    "            ## Para los casos que el campo sea 'srvaprimo' Submit_User,\n",
    "            if (str_rule_agg.strip() == last_token and \\\n",
    "                str_rule_agg.strip() not in functions_sql and \\\n",
    "                str_rule_agg.strip() not in functions_before_column and \\\n",
    "                str_rule_agg.strip() not in keywords_ignored and \\\n",
    "                flag_rule_filter == False):  \n",
    "                column_alias = token.value                 \n",
    "        \n",
    "          \n",
    "            ## Tomar el alias de la regla \n",
    "            if(flag_count_parentheses_rule and flag_rule_case == False \\\n",
    "            and token.value != subquery_alias \\\n",
    "            and (flag_rule_filter == False and flag_rule_select == False) \\\n",
    "            and last_token not in ['SELECT','AND','ON', '(', '.', '=', '+', '*',',']):\n",
    "                \n",
    "                #column_alias = last_token\n",
    "                column_alias = token.value                \n",
    "                flag_count_parentheses_rule = False                \n",
    "                \n",
    "            ## Fin del CASE con ALIAS y sin tener referencia de AS\n",
    "            if (last_token == 'END' and last_keyword == 'END'):  \n",
    "                last_token = 'AS'\n",
    "                flag_end_rule_as = True\n",
    "                flag_rule_case   = False\n",
    "            \n",
    "            \n",
    "            ## Fin de la regla con ALIAS y con AS\n",
    "            if (last_token == 'AS' and flag_count_parentheses_rule and token.value != subquery_alias):\n",
    "                column_alias = token.value\n",
    "                flag_end_rule_as = True\n",
    "            \n",
    "            \n",
    "            ## Fin del CASE con ALIAS y con AS\n",
    "            if (last_token == 'AS' and flag_end_rule_as and last_keyword != 'TABLE'):\n",
    "                \n",
    "                flag_end_rule_as = False\n",
    "                \n",
    "                # Para Ignorar los casos de CAST ('' AS CHAR(40)) AS ALIAS_CAMPO\n",
    "                if (token.value.upper() not in cast_data_type_list and token.value != subquery_alias):\n",
    "                    flag_rule_select = False\n",
    "                    column_alias     = token.value\n",
    "                    rule_select_list = [str_rule_agg, column_alias]\n",
    "                    rule_select_list_total.append(rule_select_list)\n",
    "                                        \n",
    "                    str_rule_agg = ''\n",
    "                    count_open_parentheses_rule  = 0\n",
    "                    count_close_parentheses_rule = 0\n",
    "                    flag_count_parentheses_rule = False\n",
    "                    last_keyword = 'SELECT'\n",
    "                \n",
    "\n",
    "        # Para la ultima logica antes del FROM, si no tiene un AS\n",
    "        elif (flag_rule_select and token.value == 'FROM' \\\n",
    "              and last_token not in dates_list \\\n",
    "              and flag_rule_filter == False \\\n",
    "              and count_open_parentheses_rule == 0 \\\n",
    "              and token.value != subquery_alias):\n",
    "            \n",
    "            flag_rule_select = False            \n",
    "            column_alias     = last_token\n",
    "            \n",
    "            str_rule_agg = str_rule_agg.replace(column_alias, '')\n",
    "                    \n",
    "            rule_select_list = [str_rule_agg, column_alias]\n",
    "            rule_select_list_total.append(rule_select_list)\n",
    "            str_rule_agg = ''          \n",
    "            count_open_parentheses_rule  = 0\n",
    "            count_close_parentheses_rule = 0\n",
    "            flag_count_parentheses_rule = False    \n",
    "            \n",
    "            \n",
    "        ## Apertura del WHERE, ON para los casos que vienen uno seguido de otro.\n",
    "        if (((token.is_keyword and token.value.upper() in functions_rules) or \\\n",
    "            (token is Name and token.value.upper() in functions_rules)) and \\\n",
    "            flag_rule_select == False):     \n",
    "            flag_rule_filter = True  \n",
    "\n",
    "            if (column_alias == last_token):\n",
    "                column_alias = None\n",
    "        \n",
    "        # Contar los parentesis de las reglas en un SELECT    \n",
    "        if flag_rule_select:\n",
    "            if (token.value == '('):\n",
    "                count_open_parentheses_rule += 1\n",
    "            elif (token.value == ')'):\n",
    "                count_close_parentheses_rule += 1\n",
    "                \n",
    "            if (count_open_parentheses_rule == count_close_parentheses_rule and \\\n",
    "                count_open_parentheses_rule > 0):\n",
    "                \n",
    "                flag_count_parentheses_rule = True\n",
    "                count_open_parentheses_rule  = 0\n",
    "                count_close_parentheses_rule = 0\n",
    "\n",
    "        \n",
    " \n",
    "        ## ACUMULADO DE TEXTO REGLA INTERNA\n",
    "        if ((flag_rule_select or flag_rule_filter) \\\n",
    "            and flag_end_rule_as == False and token.value != column_alias):\n",
    "            \n",
    "            if (token.ttype is Punctuation and token.value == '.'):\n",
    "                str_rule_agg = str_rule_agg + '.'\n",
    "                \n",
    "            elif (last_token in ['.']):\n",
    "                str_rule_agg = str_rule_agg + token.value \n",
    "                \n",
    "            elif (token.is_keyword and token.value.upper() in functions_rules):\n",
    "                str_rule_agg = str_rule_agg + ' '+ token.value \n",
    "                            \n",
    "            elif (token.ttype is not Text.Whitespace.Newline):\n",
    "                str_rule_agg = str_rule_agg + ' '+ token.value\n",
    "                \n",
    "                if (token.value == '('):\n",
    "                    flag_open_parentheses_rule = True \n",
    "                elif (token.value == ')'):\n",
    "                    flag_open_parentheses_rule = False \n",
    "                  \n",
    "        #print(\"\\n\\n ACUMULADO DE TEXTO REGLA INTERNA =>\"+ str_rule_agg)\n",
    "        \n",
    "                \n",
    "    \n",
    "    #######################  EXTRACCIÓN DE CAMPOS ########################################\n",
    "        '''\n",
    "        Sección que recupera los campos usados tanto en el SELECT como en los WHERE, ON \n",
    "        Retorna una lista con el formato: [CAMPO, ALIAS]\n",
    "        '''\n",
    "        if (token.ttype is Punctuation and last_token in functions_sql):\n",
    "            last_keyword = last_token\n",
    "        \n",
    "\n",
    "        # Para los casos que los campos sean 'CURRENT_TIME', etc\n",
    "        if token.is_keyword and token.value.upper() in columns_syntax_keywords \\\n",
    "           and flag_rule_select == False and flag_rule_filter == False:\n",
    "            column = token.value.upper()\n",
    "            column_list.append(column)\n",
    "\n",
    "        elif token.is_keyword and token.value.upper() not in keywords_ignored:\n",
    "            # mantiene el nombre del último valor de last keyword, ej: SELECT, FROM, WHERE, (ORDER) BY\n",
    "            last_keyword = token.value.upper()\n",
    "\n",
    "            \n",
    "        elif token.ttype is Name and token.value not in functions_sql:\n",
    "            # analiza los nombres de tokens, los nombres de columna y los valores de condiciones\n",
    "            \n",
    "            if (last_keyword in functions_before_column or last_keyword in functions_sql) \\\n",
    "               and (last_token not in ['AS'] or last_token in [',']):               \n",
    "                                \n",
    "                if token.value.upper() not in functions_sql and \\\n",
    "                   token.value.upper() not in cast_data_type_list and \\\n",
    "                   token.value.upper() != 'FROM':\n",
    "                                        \n",
    "                    if str(last_token) == '.':\n",
    "\n",
    "                        # we have table.column notation example\n",
    "                        # append column name to the last entry of columns\n",
    "                        # as it is a table name in fact\n",
    "                        table_name = column_list[-1]\n",
    "                        column_list[-1] = '{}.{}'.format(table_name, token)\n",
    "                        column = column_list[-1]\n",
    "                        \n",
    "\n",
    "                    elif last_token in ['AND'] and last_keyword == 'THEN':\n",
    "                        column = str(token.value)\n",
    "                        column_list.append(column)\n",
    "                        \n",
    "                    ## alias de campos. Ejemplo: A.Monthly_Target_Cd Monthly_Target_Cd\n",
    "                    elif ((str(column).upper() == last_token or token.value.upper() == last_token) \\\n",
    "                       and last_keyword == 'SELECT'):\n",
    "                        column_alias = token.value\n",
    "                        column_list_internal = [column, column_alias]\n",
    "                        column_alias = None\n",
    "\n",
    "                    elif (str(token.value) != subquery_alias and \\\n",
    "                          str(token.value) != column_alias ):\n",
    "                        \n",
    "                        column = str(token.value)\n",
    "                        column_list.append(column)\n",
    "\n",
    "                        \n",
    "                  \n",
    "            ## alias de campos      \n",
    "            elif (last_keyword in functions_before_column \\\n",
    "                    or last_keyword in functions_sql) \\\n",
    "                    and (last_token in ['AS'] or last_token in [',']) \\\n",
    "                    and token.value.upper() not in functions_sql: \n",
    "                column_alias = token.value\n",
    "                column_list_internal = [column, column_alias]\n",
    "                column_alias = None\n",
    "\n",
    "                    \n",
    "            elif last_keyword in ['INTO'] and token.ttype is Punctuation: #and last_token.ttype is Punctuation:\n",
    "                # INSERT INTO `foo` (col1, `col2`) VALUES (..)\n",
    "                column = str(token.value).strip('`')     \n",
    "                column_list.append(column) \n",
    "                \n",
    "        elif token.ttype is Wildcard:\n",
    "            # handle * wildcard in SELECT part, but ignore count(*)\n",
    "            # print(last_keyword, last_token, token.value)\n",
    "            if last_keyword == 'SELECT' and last_token != '(':\n",
    "                if str(last_token) == '.':\n",
    "                    # handle SELECT foo.*\n",
    "                    table_name = column_list[-1]\n",
    "                    column_list[-1] = '{}.{}'.format(table_name, str(token))\n",
    "                    column = column_list[-1]                    \n",
    "                else:\n",
    "                    column = str(token.value)  \n",
    "                    column_list.append(column)\n",
    "                    \n",
    "        length_list_column = len(column_list_internal)\n",
    "        \n",
    "        \n",
    "        if (length_list_column >= 1 and (column_list_internal not in column_list_total)):\n",
    "            column_list_total.append(column_list_internal)\n",
    "            column = None\n",
    "            column_alias = None\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "    ####################### EXTRACCIÓN DE TABLAS  ####################################        \n",
    "        '''\n",
    "        Sección que recupera las tablas de una query \n",
    "        Retorna una lista con el formato: [esquema, tabla, alias]\n",
    "        '''  \n",
    "        \n",
    "        subquery_alias    = None\n",
    "        token_value_clean = \" \".join(token.value.upper().split())\n",
    "        \n",
    "        if (token.ttype is DML or token.ttype is DDL):\n",
    "            dml_ddl_name = token.value.upper()  \n",
    "            from_clausule = False \n",
    "            \n",
    "            if(dml_ddl_name == 'CREATE'):\n",
    "                flag_create_table = True\n",
    "            \n",
    "            if(dml_ddl_name == 'SELECT' and flag_create_table):\n",
    "                flag_create_table = False\n",
    "                \n",
    "        \n",
    "        if token.ttype is Punctuation and token.value == ',' \\\n",
    "            and from_clausule == True:\n",
    "            last_keyword = 'FROM'\n",
    "        \n",
    "        if token.is_keyword and from_clausule == True and \\\n",
    "           token.value.upper() in stop_from_list:            \n",
    "            from_clausule = False\n",
    "            last_keyword = None\n",
    "        \n",
    "        \n",
    "        ## Para los casos de cerrar algunos campos en el SELECT que llaman a FROM\n",
    "        if (token.is_keyword and token.value.upper() == 'AS' \\\n",
    "           and last_token == ')' and not_get_from):\n",
    "            last_keyword = 'SELECT'\n",
    "            not_get_from = False\n",
    "            \n",
    "        \n",
    "\n",
    "        if token.is_keyword and token.value.upper() in table_syntax_keywords:\n",
    "            # keep the name of the last keyword, the next one can be a table name\n",
    "            last_keyword = \" \".join(token.value.upper().split())\n",
    "            \n",
    "            ## Para los casos de algunos campos en el SELECT que llaman a FROM\n",
    "            if token.value.upper() == 'FROM' and last_token in dates_list:\n",
    "                not_get_from = True\n",
    "                \n",
    "            elif(token.value.upper() == 'FROM' and not_get_from == True):\n",
    "                not_get_from = False\n",
    "                \n",
    "            ## Para los casos de que las tablas estén separadas por ,    \n",
    "            elif(token.value.upper() == 'FROM' and last_token in [\"'\"+'0'+\"'\"]):\n",
    "                from_clausule = False\n",
    "                not_get_from  = True\n",
    "\n",
    "            elif(token.value.upper() == 'FROM' and last_token not in[\"'\"+'0'+\"'\"]):\n",
    "                from_clausule = True\n",
    "                \n",
    "            elif(token.value.upper() in ['WHERE','GROUP']):\n",
    "                from_clausule = False\n",
    "                    \n",
    "   \n",
    "        elif str(token) == '(' and last_token not in functions_sql:\n",
    "            # reset the last_keyword for INSERT `foo` VALUES(id, bar) ...\n",
    "            #last_keyword = None\n",
    "            pass\n",
    "        elif token.is_keyword and str(token) in ['FORCE', 'ORDER']:\n",
    "            # reset the last_keyword for \"SELECT x FORCE INDEX\" queries and \"SELECT x ORDER BY\"\n",
    "            last_keyword = None\n",
    "        elif token.is_keyword and str(token) == 'SELECT' and last_keyword in ['INTO', 'TABLE']:\n",
    "            # reset the last_keyword for \"INSERT INTO SELECT\" and \"INSERT TABLE SELECT\" queries\n",
    "            last_keyword = None\n",
    "        elif token.ttype is Name or token.is_keyword:\n",
    "            # print([last_keyword, last_token, token.value])\n",
    "            # analyze the name tokens, column names and where condition values\n",
    "\n",
    "            if token.ttype is Name and token.value in functions_ignored \\\n",
    "               and from_clausule == True :\n",
    "                not_get_from = True\n",
    "                    \n",
    "                  \n",
    "\n",
    "            ## De esta lista quité el 'INTO'\n",
    "            if last_keyword in ['FROM', 'JOIN', 'INNER JOIN', 'LEFT JOIN', 'RIGHT JOIN', \n",
    "                                'LEFT OUTER JOIN','RIGHT OUTER JOIN', 'TABLE'] \\\n",
    "                and last_token not in ['AS', 'CREATE'] \\\n",
    "                and token.value not in ['AS', 'SELECT'] \\\n",
    "                and not_get_from == False \\\n",
    "                and flag_create_table == False \\\n",
    "                and flag_rule_filter == False:\n",
    "\n",
    "                if last_token == '.':\n",
    "                    # we have database.table notation example\n",
    "                    # append table name to the last entry of tables\n",
    "                    # as it is a database name in fact\n",
    "                    #print(\"tables[-1] -------------------> \" + str(tables))\n",
    "                    if (len(tables) >= 1):\n",
    "\n",
    "                        database_name = tables[-1]\n",
    "                        table = token.value\n",
    "                        tables[-1] = '{}.{}'.format(database_name, token)\n",
    "                        last_keyword = 'TABLE' #None\n",
    "                        table_list = [database_name, table]\n",
    "                        \n",
    "                        if (dml_ddl_name not in ['DROP']):\n",
    "                            table_list_total.append(table_list)\n",
    "                    \n",
    "                elif last_token not in [',', last_keyword] and \\\n",
    "                     last_token not in ['ON', last_keyword] and \\\n",
    "                     len(table_list) > 0:\n",
    "                    # it's not a list of tables, e.g. SELECT * FROM foo, bar\n",
    "                    # hence, it can be the case of alias without AS, e.g. SELECT * FROM foo bar\n",
    "                    alias_table = token.value\n",
    "                    table_list.append(alias_table)\n",
    "\n",
    "                else: ## database schema\n",
    "                    table_name = str(token.value.strip('`'))\n",
    "                    tables.append(table_name)\n",
    "            \n",
    "            elif(last_token == 'AS' and len(table_list) > 0):\n",
    "                alias_table = token.value\n",
    "                table_list.append(alias_table)\n",
    "\n",
    "        length_list_table = len(table_list)\n",
    "\n",
    "        if (length_list_table >= 2 and (table_list not in table_list_total)):\n",
    "            table_list_total.append(table_list)\n",
    "            \n",
    "            \n",
    "            \n",
    "        #Variables que acumulan ultimos valores del token y ayudan a analizar la query\n",
    "        flag_lastjoin_is_subquery = flag_open_parentheses_subq\n",
    "    \n",
    "        if (token.ttype is Text.Whitespace.Newline):\n",
    "            last_token = last_token.upper() \n",
    "        else:\n",
    "            last_token = token.value.upper()\n",
    "  \n",
    "        \n",
    "    ## CUANDO TERMINA EL TOKEN, PODRÍA QUEDAR LÓGICA POR FUERA DESPUÉS DEL ;\n",
    "    if (rule_generic_list_total and df_final_columns.empty == False): \n",
    "                \n",
    "        df_final_columns = \\\n",
    "                       updateInDataFrameColumnsRules(df_final_columns, rule_generic_list_total) \n",
    "    \n",
    "\n",
    "    if (df_final_tables.empty == False or df_final_columns.empty == False):\n",
    "        df_merge_table_column_input = mergeDataframeTablesColumnsQuery(df_final_tables, df_final_columns)\n",
    "    \n",
    "    return df_merge_table_column_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones: Insertar en Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10501,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emptyDataframeExtractQuery(file_type):\n",
    "    '''\n",
    "    Función que retorna un objeto dataframe vacío para obtener las tablas, campos y reglas de una query    \n",
    "    Formato ['TIPO_ARCHIVO','ARCHIVO','LINEAS_DE_CODIGO','NUMERO_PASO','SENTENCIA_DML', \n",
    "             'ESQUEMA_OUTPUT','TABLA_OUTPUT','MULTINIVEL','ESQUEMA_INPUT','TABLA_INPUT',\n",
    "             'ALIAS_SUBQUERY', 'COLUMNA','VARIABLE', 'LOGICA_NEGOCIO', 'ALIAS_CAMPO']\n",
    "    '''\n",
    "    columnsDF = []\n",
    "\n",
    "    if (file_type in ['Bteq','StoredProcedures']):\n",
    "        columnsDF = ['TIPO_ARCHIVO','ARCHIVO','LINEAS_DE_CODIGO','NUMERO_PASO','SENTENCIA_DML', \n",
    "                     'ESQUEMA_OUTPUT','TABLA_OUTPUT','MULTINIVEL','ESQUEMA_INPUT','TABLA_INPUT',\n",
    "                     'ALIAS_SUBQUERY', 'COLUMNA','VARIABLE', 'LOGICA_NEGOCIO', 'ALIAS_CAMPO']\n",
    "\n",
    "    elif (file_type == 'Vistas'):  \n",
    "        columnsDF = ['TIPO_ARCHIVO','ARCHIVO','LINEAS_DE_CODIGO','SENTENCIA_DML','ESQUEMA_OUTPUT','TABLA_OUTPUT',\n",
    "                     'MULTINIVEL', 'ESQUEMA_INPUT', 'TABLA_INPUT','ALIAS_SUBQUERY', 'COLUMNA',\n",
    "                     'VARIABLE', 'LOGICA_NEGOCIO', 'ALIAS_CAMPO'] \n",
    "\n",
    "    df = pd.DataFrame(columns = columnsDF) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10502,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emptyDataframeTablesQuery(data):\n",
    "    '''\n",
    "    Función que retorna un objeto dataframe vacío para obtener las tablas de una query\n",
    "    Formato ['MULTINIVEL','ESQUEMA_INPUT','TABLA_INPUT','ALIAS_TABLA','ALIAS_SUBQUERY']\n",
    "    '''\n",
    "    \n",
    "    columnsDF = ['MULTINIVEL','ESQUEMA_INPUT','TABLA_INPUT','ALIAS_TABLA', 'ALIAS_SUBQUERY']\n",
    "    \n",
    "    if data is not None:\n",
    "        df = pd.DataFrame(columns = columnsDF) \n",
    "    else:\n",
    "        df = pd.DataFrame(data,columns = columnsDF) \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10503,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emptyDataframeColumnsQuery(data):\n",
    "    '''\n",
    "    Función que retorna un objeto dataframe vacío para obtener los campos y reglas de una query    \n",
    "    Formato ['MULTINIVEL','CAMPOS','LOGICA_NEGOCIO','ALIAS_CAMPO']\n",
    "    '''\n",
    "    \n",
    "    columnsDF = ['MULTINIVEL','CAMPOS','LOGICA_NEGOCIO','ALIAS_CAMPO']\n",
    "    \n",
    "    if data is not None:\n",
    "        df = pd.DataFrame(columns = columnsDF) \n",
    "    else:\n",
    "        df = pd.DataFrame(data,columns = columnsDF) \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emptyDataframeTablesColumnsInputQuery(data):\n",
    "    '''\n",
    "    Función que retorna un objeto dataframe vacío para obtener los campos y reglas de una query    \n",
    "    Formato ['MULTINIVEL','ESQUEMA_INPUT','TABLA_INPUT','ALIAS_SUBQUERY',\n",
    "             'NOMBRE_CAMPO', 'CAMPO_COMPUESTO','LOGICA_NEGOCIO','ALIAS_CAMPO']\n",
    "    '''\n",
    "    \n",
    "    columnsDF = ['MULTINIVEL','ESQUEMA_INPUT','TABLA_INPUT','ALIAS_SUBQUERY',\n",
    "                 'NOMBRE_CAMPO', 'CAMPO_COMPUESTO','LOGICA_NEGOCIO','ALIAS_CAMPO']\n",
    "    \n",
    "    if data is not None:\n",
    "        df = pd.DataFrame(columns = columnsDF) \n",
    "    else:\n",
    "        df = pd.DataFrame(data,columns = columnsDF) \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de procesamiento de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10505,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files_csv():\n",
    "    '''\n",
    "    Función que elimina del directorio local los archivos antiguos de levantamiento\n",
    "    '''\n",
    "    for filename in glob.glob(\"./LAC_Levantamiento_*.csv\"):\n",
    "        os.remove(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_scandir_files(path):\n",
    "    '''\n",
    "    Función que escanea los directorios y subdirectorios,\n",
    "    obteniendo como resultado una lista de archivo con,\n",
    "    formato: ./Directorio/Subdirectorio/archivo\n",
    "    '''\n",
    "\n",
    "    file_list = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if(file.endswith(\".sql\") or file.endswith('.txt')):\n",
    "                file_list.append(os.path.join(root,file))\n",
    "                \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10507,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_contains_name(folder_dir):\n",
    "    '''\n",
    "    Función que retorna que tipo de archivo se está procesando\n",
    "    '''\n",
    "    \n",
    "    if (folder_dir.find('Bteq') != -1 or folder_dir.find('BTEQ') != -1):\n",
    "        return 'Bteq'\n",
    "    if (folder_dir.find('Vistas') != -1):\n",
    "        return 'Vistas'\n",
    "    if (folder_dir.find('StoredProcedures') != -1):\n",
    "        return 'StoredProcedures'\n",
    "    \n",
    "    return 'Bteq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10508,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_file_errors(MyFile, file_type, archivo_no_procesado):\n",
    "    '''\n",
    "    Función que inserta en un archivo que por alguna razón da error\n",
    "    '''\n",
    "    MyFile.write(\"%s\" % 'Archivo tipo *'+ str(file_type) + '*: '+archivo_no_procesado+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10509,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_dataframe_in_csv(df, file_type):\n",
    "    '''\n",
    "    Función que crea un archivo de levantamiento desde un dataframe\n",
    "    '''\n",
    "    \n",
    "    filename = 'LAC_Levantamiento_'+str(file_type)+'.csv'\n",
    "    #df.to_csv(filename, sep='|', index=None, encoding='utf-8')    \n",
    "    \n",
    "    with open(filename, 'a') as f:\n",
    "        df.to_csv(f, sep='|', index=None, header=f.tell()==0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10510,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_files(folder, filename):\n",
    "    '''\n",
    "    Función que abre un archivo y lo procesa linea por linea, eliminando sus comentarios \n",
    "    y retornando las querys a procesar en una lista\n",
    "    '''\n",
    "\n",
    "    queryVista           = ''    \n",
    "    \n",
    "    ## Lista de sintaxis SQL que debería tomar cada linea del archivo,\n",
    "    ## contiene espacio al final porque hay campos que pueden llamarse \"Update_dttm\"\n",
    "    ## y tomarlo como una palabra reservada\n",
    "    dml_dll_syntax_query = [\n",
    "        'INSERT', 'CREATE ', \n",
    "        'UPDATE ', \n",
    "        'DROP ', 'DELETE ','DELETE',\n",
    "        'RENAME '\n",
    "    ]\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        \n",
    "        archivo         = open(filename, encoding=\"utf8\",errors='ignore')\n",
    "        textoArchivo    = archivo.read()\n",
    "        listaQuery      = []\n",
    "        sum_lines       = ''\n",
    "        \n",
    "        ## BTEQ Y STORED PROCEDURES \n",
    "        if (folder in ['Bteq','StoredProcedures']):\n",
    "            \n",
    "            bteq_text = re.sub('\\-\\-.*?\\n|\\/\\*.*?\\*\\/', ' ', textoArchivo)  \n",
    "                                    \n",
    "            bteq_text = remove_comments(bteq_text)\n",
    "                        \n",
    "            lines = bteq_text.splitlines();\n",
    "            \n",
    "            code_lines = str(len(lines))\n",
    "            \n",
    "            for line in lines: \n",
    "                # elimina espacios a comienzo de la linea y lo edita a mayuscula\n",
    "                line = line.strip().upper()                \n",
    "                                    \n",
    "                # comienza agregando las querys que interesan\n",
    "                if line.startswith(tuple(dml_dll_syntax_query)): \n",
    "                    sum_lines = line\n",
    "                    if line.endswith(\";\"):\n",
    "                        listaQuery.append(line)\n",
    "                        sum_lines = ''                        \n",
    "                        \n",
    "                # insertar en lista cuando termina la query\n",
    "                elif (line.endswith(\";\") and sum_lines != ''): \n",
    "                    sum_lines = sum_lines + ' '+ line\n",
    "                    listaQuery.append(sum_lines)  \n",
    "                    sum_lines = ''      \n",
    "                    \n",
    "                # acumula en una variable hasta conseguir un ;\n",
    "                elif (sum_lines != ''):                \n",
    "                    sum_lines = sum_lines + ' '+ line\n",
    "                           \n",
    "        ## VIEWS\n",
    "        elif (folder == 'Vistas'):\n",
    "            # LIMPIA EL CODIGO SQL PARA QUE NO TENGA NINGUN TIPO DE COMENTARIO\n",
    "            queryVista = re.sub('\\-\\-.*?\\n|\\/\\*.*?\\*\\/|(--[^\\r\\n]*$)', ' ', textoArchivo)   \n",
    "\n",
    "            queryVista = remove_comments(queryVista)\n",
    "                        \n",
    "            lines = queryVista.splitlines();\n",
    "            \n",
    "            code_lines = str(len(lines))\n",
    "            \n",
    "            queryVista = queryVista.replace('\\\\', '')  \n",
    "            queryVista = queryVista.replace('\"', '') \n",
    "            \n",
    "            queryVista = queryVista + \";\"\n",
    "            \n",
    "            listaQuery.append(queryVista)\n",
    "   \n",
    "    return listaQuery, code_lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INICIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10511,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_folders_files(ppal_dir):      \n",
    "    '''\n",
    "    Función que itera en los directorios para obtener los archivos que se \n",
    "    quieren procesar (bteq, sp, vistas)\n",
    "    '''\n",
    "    \n",
    "    ## Elimina los archivos de levantamiento\n",
    "    remove_files_csv()\n",
    "    \n",
    "    MyFile=open('archivos_no_procesados.txt','w')\n",
    "    \n",
    "    folder_dir       = None      # Ruta completa de cada carpeta a leer\n",
    "    file_dir         = None      # Ruta completa del archivo\n",
    "    result_folder    = None \n",
    "    code_lines       = None\n",
    "    \n",
    "    # para pruebas   \n",
    "    folder_file2='./ArchivosProcesar/StoredProcedures/SPs Capacity Flexible.sql'\n",
    "\n",
    "    \n",
    "    ## Obtiene los nombres de los archivo de extracción con sus carpetas y subcarpetas.    \n",
    "    folder_list = fast_scandir_files(ppal_dir)\n",
    "    folder_list.sort()\n",
    "    \n",
    "    for folder_file in folder_list:\n",
    "        \n",
    "        ## Retorna nombre corto del tipo de archivo que se está procesando (Bteq, Vista, StoredProcedure)\n",
    "        result_folder      = folder_contains_name(folder_file)\n",
    "        df_by_file_querys  = emptyDataframeExtractQuery(result_folder)\n",
    "     \n",
    "        #if (folder_file.find('Bteq') != -1 or folder_file.find('Input_Analitico') != -1): \n",
    "        #if (folder_file.find('Vistas') != -1):         \n",
    "        if (folder_file == folder_file):\n",
    "            if result_folder is not None:\n",
    "\n",
    "                print(folder_file)\n",
    "\n",
    "                listaQuerys, code_lines = processing_files(result_folder, folder_file)\n",
    "\n",
    "                if(len(listaQuerys) == 0):\n",
    "                    insert_file_errors(MyFile, result_folder, folder_file)\n",
    "\n",
    "                process_number = 0\n",
    "\n",
    "                for queryString in listaQuerys:\n",
    "                    try:\n",
    "                        if (queryString):    \n",
    "\n",
    "                            process_number += 1                    \n",
    "\n",
    "                            queryString    = preprocess_query(queryString) \n",
    "\n",
    "                            #print(str(process_number) + \" = \" + queryString)\n",
    "\n",
    "                            #if (process_number == 68):\n",
    "                                #print(queryString)\n",
    "\n",
    "                            # Retorna todas las tablas input y output en un dataframe\n",
    "                            df_internal_merge  = get_output_input_tables(result_folder, folder_file, \\\n",
    "                                                            code_lines, queryString, process_number)\n",
    "\n",
    "                            if (df_internal_merge.empty == False):\n",
    "                                df_by_file_querys  = df_by_file_querys.append(df_internal_merge, sort=False)\n",
    "\n",
    "                \n",
    "                    except Exception as e: \n",
    "                        print(\"Error with file: \"+folder_file)\n",
    "                        print(e)\n",
    "                        insert_file_errors(MyFile, result_folder, folder_file)\n",
    "                        pass  \n",
    "                    \n",
    "                # Inserta el conjunto completo a los archivos .csv\n",
    "                insert_dataframe_in_csv(df_by_file_querys, result_folder)\n",
    "          \n",
    "    MyFile.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10513,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-08 13:26:59.906354\n",
      "ELIMINANDO Y CREANDO TABLAS..\n",
      "INSERTANDO ARCHIVOS A LAS TABLAS..\n",
      "\n",
      "2020-05-08 13:40:18.215084\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    now = datetime.datetime.now()\n",
    "    print (now)\n",
    "    \n",
    "    # Ruta principal donde comienza la extracción\n",
    "    ppal_dir        = './ArchivosProcesar'       \n",
    "    \n",
    "    ## Recorre todas las carpetas y subcarpetas para extraer los archivos \n",
    "    loop_folders_files(ppal_dir)\n",
    "\n",
    "    # Elimina y crea las tablas en Teradata    \n",
    "    create_and_drop_tables_teradata()\n",
    "        \n",
    "    # Inserta el contenido de los archivos a las tablas en Teradata\n",
    "    insert_files_csv_in_teradata()\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    print (now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
