{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levantamiento Tablas Acciones Comerciales - Querys de CIM desde Teradata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **PROYECTO : LEVANTAMIENTO CÓDIGO CIM EN EL GESTOR DE CAMPANAS TERADATA** <br> \n",
    "**Extracción de tablas utilizadas directamente desde CIM según una lista de comunicaciones específica** <br>\n",
    "El archivo __query_segmentos_CIM_prod.txt__ tiene la query que extrae las comunicaciones, segmentaciones y su ultima query ejecutada <br>\n",
    "Versión:  1.0  <br>\n",
    "Fecha: 01-05-2020  <br>\n",
    "Descripción: Versión Inicial  <br>\n",
    "Desarrollador: Axity | Adriana Jiménez "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "## Librerias de parseo de queries\n",
    "import sqlparse\n",
    "from sqlparse.sql import TokenList\n",
    "from sqlparse.tokens import Name, Whitespace, Wildcard, Number, Punctuation, Text, Operator\n",
    "from sqlparse.tokens import DML, DDL, Keyword\n",
    "import sql_metadata as sqllib\n",
    "\n",
    "## Libreria que se integra con Teradata\n",
    "import giraffez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones: Insertar en BD Teradata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuracion de conexion\n",
    "td_config = {\n",
    "    \"username\": \"exajibl\",\n",
    "    \"password\": \"acjb0610\",\n",
    "    \"host\": \"dataware.bci.cl\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_drop_tables_teradata():\n",
    "    '''\n",
    "    Función que crea las tablas en Teradata\n",
    "    '''\n",
    "    \n",
    "    print(\"ELIMINANDO Y CREANDO TABLAS..\\n\")\n",
    "    \n",
    "    drop_sql_bteq    = \"DROP TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_CIM\"\n",
    "\n",
    "    create_sql_bteq = \"\"\"CREATE MULTISET TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_CIM\n",
    "        (\n",
    "          NOMBRE_COMUNICACION VARCHAR(100) ,\n",
    "          NOMBRE_SEGMENTO VARCHAR(50),\n",
    "          ESQUEMA_INPUT VARCHAR(50) ,\n",
    "          TABLA_INPUT VARCHAR(50)      \n",
    "          ) ;\"\"\"\n",
    "\n",
    "\n",
    "    with giraffez.Cmd(**td_config) as cmd:\n",
    "        if cmd.exists(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_CIM\"):\n",
    "            cmd.execute(drop_sql_bteq)\n",
    "\n",
    "        cmd.execute(create_sql_bteq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_files_csv_in_teradata():\n",
    "    '''\n",
    "    Función que inserta los dataframes a Teradata\n",
    "    '''\n",
    "    \n",
    "    print(\"INSERTANDO ARCHIVOS A LAS TABLAS..\\n\")\n",
    "    \n",
    "    df_cim = pd.read_csv(\"LAC_Levantamiento_Tablas_CIM.csv\",  sep='|' )  \n",
    "\n",
    "    if (df_cim.empty == False): \n",
    "        with giraffez.BulkLoad(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_CIM\", **td_config) as load:\n",
    "            load.cleanup()\n",
    "            load.columns = df_cim.columns.tolist()\n",
    "            for row in df_cim.values.tolist(): \n",
    "                load.put([row[0], row[1], row[2], row[3] ])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones modificadas de librería sql_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(_list):\n",
    "    \"\"\"\n",
    "    Hace que una lista tenga registro unicos y mantengan el orden\n",
    "    \"\"\"\n",
    "    \n",
    "    ret = []\n",
    "\n",
    "    for item in _list:\n",
    "        if item not in ret:\n",
    "            ret.append(item)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_tables(query):\n",
    "    \"\"\"\n",
    "    Función que retorna una lista de tablas INPUT de una query\n",
    "    \"\"\"\n",
    "    tables              = []\n",
    "    tables_2            = []\n",
    "    table_list          = []\n",
    "    table_list_total    = []\n",
    "    last_keyword  = None\n",
    "    last_token    = None\n",
    "    dml_ddl_name  = None\n",
    "    not_get_from  = False\n",
    "    from_clausule = False\n",
    "\n",
    "\n",
    "    df_internal_table   = emptyDataframeTablesQuery()\n",
    "    \n",
    "    #     \n",
    "    dates_list = ['SECOND', 'MINUTE', 'HOUR', 'DAY', 'YEAR','MONTH','BOTH', 'TRAILING']\n",
    "\n",
    "    \n",
    "    stop_from_list = ['AS','CASE','WHEN','ON','AND','END','WHERE','GROUP','OVER','PARTITION','SET']\n",
    "    \n",
    "    functions_ignored = ['COUNT', 'MIN', 'MAX', 'SUM', 'FROM_UNIXTIME', 'DEC', \n",
    "    'CAST', 'CONVERT', 'ZEROIFNULL','SUBSTR','SUBSTRING','ROW_NUMBER','QUALIFY', 'ADD_MONTHS',\n",
    "    'COALESCE', 'CHAR', 'INTEGER', 'TRIM', 'OVER', 'FORMAT', 'DATE_FORMAT',\n",
    "    'CHAR_LENGTH']\n",
    "\n",
    "    # Lista de funciones reservadas de SQL ANSI para las uniones de tablas\n",
    "    table_syntax_joins = ['FROM','JOIN', 'INNER JOIN', 'LEFT JOIN', 'LEFT OUTER JOIN', \n",
    "    'RIGHT JOIN', 'RIGHT OUTER JOIN']\n",
    "\n",
    "    table_syntax_keywords = [\n",
    "        # SELECT queries\n",
    "        'FROM', 'WHERE', 'JOIN', 'INNER JOIN', 'LEFT JOIN', 'LEFT OUTER JOIN', \n",
    "        'RIGHT JOIN', 'RIGHT OUTER JOIN', 'ON',\n",
    "        \n",
    "        # INSERT queries\n",
    "        'INTO', 'VALUES',\n",
    "        # UPDATE queries\n",
    "        'UPDATE', 'SET',\n",
    "        # Hive queries\n",
    "        'TABLE',  # INSERT TABLE\n",
    "    ]\n",
    "    \n",
    "    archivo_caido = 'SQLAExport1383.txt'\n",
    "    \n",
    "    for token in sqllib.get_query_tokens(query):\n",
    "        \n",
    "        token_value_clean = \" \".join(token.value.upper().split())\n",
    "        \n",
    "        ## Homologar en caso que se consiga un 'SEL' en vez de 'SELECT'\n",
    "        if ((token.ttype is Name and token.value.upper() == 'SEL')):\n",
    "            token.ttype = DML\n",
    "            token.value = 'SELECT'\n",
    "            last_keyword = 'SELECT'\n",
    "            \n",
    "        if (token.ttype is DML or token.ttype is DDL):\n",
    "            dml_ddl_name = token.value.upper()  \n",
    "            from_clausule = False      \n",
    "        \n",
    "        if token.ttype is Punctuation and token.value == ',' \\\n",
    "            and from_clausule == True:\n",
    "            last_keyword = 'FROM'\n",
    "        \n",
    "        if token.is_keyword and from_clausule == True and \\\n",
    "           token.value.upper() in stop_from_list:            \n",
    "            from_clausule = False\n",
    "            last_keyword = None\n",
    "                        \n",
    "        if token.is_keyword and token_value_clean in table_syntax_keywords:\n",
    "            # keep the name of the last keyword, the next one can be a table name\n",
    "            last_keyword = \" \".join(token.value.upper().split())\n",
    "            \n",
    "            ## Para los casos de algunos campos en el SELECT que llaman a FROM\n",
    "            if token.value.upper() == 'FROM' and last_token in dates_list:\n",
    "                not_get_from = True\n",
    "                \n",
    "            elif(token.value.upper() == 'FROM' and not_get_from == True):\n",
    "                not_get_from = False\n",
    "                \n",
    "            ## Para los casos de que las tablas estén separadas por ,    \n",
    "            elif(token.value.upper() == 'FROM' and last_token in [\"'\"+'0'+\"'\"]):\n",
    "                from_clausule = False\n",
    "                not_get_from  = True\n",
    "\n",
    "            elif(token.value.upper() == 'FROM' and last_token not in[\"'\"+'0'+\"'\"]):\n",
    "                from_clausule = True\n",
    "\n",
    "                \n",
    "            elif(token.value.upper() in ['WHERE','GROUP']):\n",
    "                from_clausule = False\n",
    "            \n",
    "        elif str(token) == '(' or str(token) == ')':\n",
    "            #print(\" ENTRO 2 \" )\n",
    "            # reset the last_keyword for INSERT `foo` VALUES(id, bar) ...\n",
    "            last_keyword = None\n",
    "        elif token.is_keyword and str(token) in ['FORCE', 'ORDER']:\n",
    "            #print(\" ENTRO 3 \" )\n",
    "            # reset the last_keyword for \"SELECT x FORCE INDEX\" queries and \"SELECT x ORDER BY\"\n",
    "            last_keyword = None\n",
    "        elif token.is_keyword and str(token) == 'SELECT' and last_keyword in ['INTO', 'TABLE']:\n",
    "            # reset the last_keyword for \"INSERT INTO SELECT\" and \"INSERT TABLE SELECT\" queries\n",
    "            last_keyword = None\n",
    "            #print(\" ENTRO 4 \" )\n",
    "        elif token.ttype is Name or token.is_keyword:\n",
    "            # print([last_keyword, last_token, token.value])\n",
    "            # analyze the name tokens, column names and where condition values\n",
    "\n",
    "            if token.ttype is Name and token.value in functions_ignored \\\n",
    "               and from_clausule == True :\n",
    "                not_get_from = True\n",
    "\n",
    "\n",
    "            if last_keyword in ['FROM', 'JOIN', 'INNER JOIN', 'LEFT JOIN', 'RIGHT JOIN',\n",
    "                                'LEFT OUTER JOIN','RIGHT OUTER JOIN', 'UPDATE' ] \\\n",
    "                    and last_token not in ['AS', 'CREATE'] \\\n",
    "                    and token.value not in ['AS', 'SELECT'] \\\n",
    "                    and not_get_from == False:\n",
    "\n",
    "                if last_token == '.':\n",
    "                    \n",
    "                    # we have database.table notation example\n",
    "                    # append table name to the last entry of tables\n",
    "                    # as it is a database name in fact\n",
    "                    database_name = tables[-1]\n",
    "                    table = token.value\n",
    "                    tables[-1] = '{}.{}'.format(database_name, token)\n",
    "                    last_keyword = None\n",
    "                    \n",
    "                    table_list = [database_name, table]\n",
    "                    if (dml_ddl_name not in ['DROP']):\n",
    "                        table_list_total.append(table_list)\n",
    "\n",
    "\n",
    "                elif last_token not in [',', last_keyword] and \\\n",
    "                    last_keyword not in table_syntax_joins:\n",
    "                    # it's not a list of tables, e.g. SELECT * FROM foo, bar\n",
    "                    # hence, it can be the case of alias without AS, e.g. SELECT * FROM foo bar\n",
    "                    pass\n",
    "                else: #esquema \n",
    "                        \n",
    "                    table_name = str(token.value.strip('`'))\n",
    "                    tables.append(table_name)\n",
    "                \n",
    "                             \n",
    "        last_token = token.value.upper()\n",
    "\n",
    "    if (len(table_list_total) == 0):\n",
    "        result = [ noConditionInput, noConditionInput ]\n",
    "        table_list_total.append(result)\n",
    "    \n",
    "    return unique(table_list_total)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de procesamiento de query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comments(text):\n",
    "    \"\"\" remove c-style comments.\n",
    "        text: blob of text with comments (can include newlines)\n",
    "        returns: text with comments removed\n",
    "    \"\"\"\n",
    "    pattern = r\"\"\"\n",
    "                            ##  --------- COMMENT ---------\n",
    "           /\\*              ##  Start of /* ... */ comment\n",
    "           [^*]*\\*+         ##  Non-* followed by 1-or-more *'s\n",
    "           (                ##\n",
    "             [^/*][^*]*\\*+  ##\n",
    "           )*               ##  0-or-more things which don't start with /\n",
    "                            ##    but do end with '*'\n",
    "           /                ##  End of /* ... */ comment\n",
    "         |                  ##  -OR-  various things which aren't comments:\n",
    "           (                ## \n",
    "                            ##  ------ \" ... \" STRING ------\n",
    "             \"              ##  Start of \" ... \" string\n",
    "             (              ##\n",
    "               \\\\.          ##  Escaped char\n",
    "             |              ##  -OR-\n",
    "               [^\"\\\\]       ##  Non \"\\ characters\n",
    "             )*             ##\n",
    "             \"              ##  End of \" ... \" string\n",
    "           |                ##  -OR-\n",
    "                            ##\n",
    "                            ##  ------ ' ... ' STRING ------\n",
    "             '              ##  Start of ' ... ' string\n",
    "             (              ##\n",
    "               \\\\.          ##  Escaped char\n",
    "             |              ##  -OR-\n",
    "               [^'\\\\]       ##  Non '\\ characters\n",
    "             )*             ##\n",
    "             '              ##  End of ' ... ' string\n",
    "           |                ##  -OR-\n",
    "                            ##\n",
    "                            ##  ------ ANYTHING ELSE -------\n",
    "             .              ##  Anything other char\n",
    "             [^/\"'\\\\]*      ##  Chars which doesn't start a comment, string\n",
    "           )                ##    or escape\n",
    "    \"\"\"\n",
    "    regex = re.compile(pattern, re.VERBOSE|re.MULTILINE|re.DOTALL)\n",
    "    noncomments = [m.group(2) for m in regex.finditer(text) if m.group(2)]\n",
    "\n",
    "    return \"\".join(noncomments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteInconsistencyQuery(queryString):\n",
    "    '''\n",
    "    Función que elimina caracteres varios de una query\n",
    "    '''    \n",
    "    characters = '[%#${}]'\n",
    "    finalQuery = re.sub(characters, '', str(queryString))\n",
    "    return finalQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones: Insertar en Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_dataframe_in_csv(df):\n",
    "    '''\n",
    "    Función que crea un archivo de levantamiento desde un dataframe\n",
    "    '''\n",
    "    \n",
    "    with open('LAC_Levantamiento_Tablas_CIM.csv', 'a') as f:\n",
    "        df.to_csv(f, sep='|', index=None, header=f.tell()==0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emptyDataframeTablesQuery():\n",
    "    '''\n",
    "    Función que retorna un objeto dataframe vacío para obtener las tablas de una query\n",
    "    \n",
    "    Formato ['NOMBRE_COMUNICACION','NOMBRE_SEGMENTO','ESQUEMA_INPUT','TABLA_INPUT']\n",
    "    '''\n",
    "    \n",
    "    columnsDF = ['NOMBRE_COMUNICACION','NOMBRE_SEGMENTO','ESQUEMA_INPUT','TABLA_INPUT']\n",
    "\n",
    "    df = pd.DataFrame(columns = columnsDF) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertDataframeTablesQuery(communication, segment, input_tables_list):\n",
    "    '''\n",
    "    Función que inserta en un dataframe los objetos de una query\n",
    "    '''\n",
    "\n",
    "    new_df_tables = emptyDataframeTablesQuery()\n",
    "    \n",
    "    if (input_tables_list):\n",
    "        for item in input_tables_list:\n",
    "            \n",
    "            new_df_tables = new_df_tables.append(\n",
    "                [ { 'NOMBRE_COMUNICACION' : communication, \n",
    "                    'NOMBRE_SEGMENTO'   : segment,\n",
    "                    'ESQUEMA_INPUT'  : item[0],\n",
    "                    'TABLA_INPUT'    : item[1] }], \n",
    "                    ignore_index=True, sort=False) \n",
    "   \n",
    "    return new_df_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query desde TERADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_querys_cim_teradata():\n",
    "    '''\n",
    "    Función que busca en Teradata las querys a procesar según comunicación y segmentación\n",
    "    '''\n",
    "\n",
    "    filename        = 'query_segmentos_CIM_prod.txt'\n",
    "    archivo         = open(filename, encoding=\"utf8\",errors='ignore')\n",
    "    query_segments  = archivo.read()\n",
    "    \n",
    "    with giraffez.BulkExport(query_segments, **td_config) as export:\n",
    "        dataset = export.to_list()\n",
    "        \n",
    "    headers = ['NOMBRE_COMUNICACION','SEGMENTO', 'QUERY_CIM', 'ULTIMA_FECHA']\n",
    "    #df = pd.DataFrame(headers, dataset)\n",
    "\n",
    "    df = pd.DataFrame(dataset, columns = headers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    print (now)\n",
    "    \n",
    "    # Obtiene en un dataframe la información\n",
    "    df = get_querys_cim_teradata()\n",
    "    query_cim_list = df.values.tolist()\n",
    "   \n",
    "    df_internal_table_file = emptyDataframeTablesQuery()\n",
    "\n",
    "\n",
    "    # Recorre las comunicaciones -> segmentaciones -> query \n",
    "    for query in query_cim_list:\n",
    "\n",
    "        communication = query[0]\n",
    "        segment       = query[1]\n",
    "        queryString   = query[2]\n",
    "\n",
    "        queryString = queryString.replace('\"','')\n",
    "\n",
    "        queryString = deleteInconsistencyQuery(queryString)\n",
    "\n",
    "        queryString = re.sub('\\-\\-.*?\\n|\\/\\*.*?\\*\\/', ' ', queryString)  \n",
    "\n",
    "        queryString = remove_comments(queryString)\n",
    "\n",
    "        # Retorna todas las tablas input\n",
    "        input_tables_list = get_query_tables(queryString)\n",
    "\n",
    "        # Retorna todas las tablas input en forma de dataframe con su comunicacion y segmentacion\n",
    "        df_internal_table = insertDataframeTablesQuery(communication, segment, input_tables_list)\n",
    "\n",
    "        # Acumula los df en uno final\n",
    "        if (df_internal_table.empty == False):\n",
    "            df_internal_table_file = df_internal_table_file.append(df_internal_table)\n",
    "\n",
    "    \n",
    "    # Inserta todo el analisis de extracción de tablas a un archivo .CSV\n",
    "    insert_dataframe_in_csv(df_internal_table_file)  \n",
    "    \n",
    "    '''\n",
    "    Sección donde crea e inserta en la tabla temporal de Teradata la información obtenida\n",
    "    '''\n",
    "        \n",
    "    # Elimina y crea las tablas en Teradata\n",
    "    create_and_drop_tables_teradata()\n",
    "    \n",
    "    # Inserta el contenido de los archivos a las tablas en Teradata\n",
    "    insert_files_csv_in_teradata()\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    print (now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
