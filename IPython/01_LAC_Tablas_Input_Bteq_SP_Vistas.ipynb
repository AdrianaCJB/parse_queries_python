{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Levantamiento Tablas Acciones Comerciales - Código"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **PROYECTO : LEVANTAMIENTO CÓDIGO EN EL GESTOR DE CAMPANAS TERADATA** <br> \n",
    "**Extracción de tablas utilizadas en las Vistas, Bteq, SP e Input Analítico** <br>\n",
    "Los archivos a procesar están en la carpeta ./ArchivosProcesar  <br>\n",
    "Versión:  1.0  <br>\n",
    "Fecha: 01-05-2020  <br>\n",
    "Descripción: Versión Inicial  <br>\n",
    "Desarrollador: Axity | Adriana Jiménez "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "* El siguiente desarrollo está basado en la librería de python sql_metadata\n",
    "* Modificando la función 'get_query_tables' según las necesidades específicas de este requerimiento\n",
    "\n",
    "URLs:\n",
    "https://pypi.org/project/sql_metadata/\n",
    "https://github.com/macbre/sql-metadata/blob/master/sql_metadata.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\npip install pandas\\npip install sql_metadata\\npip install sqlparse\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "pip install pandas\n",
    "pip install sql_metadata\n",
    "pip install sqlparse\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os, glob\n",
    "import pandas as pd \n",
    "from decimal import Decimal as D\n",
    "import datetime\n",
    "\n",
    "## Librerias de parseo de queries\n",
    "import sqlparse\n",
    "from sqlparse.sql import TokenList\n",
    "from sqlparse.tokens import Name, Whitespace, Wildcard, Number, Punctuation, Text, Operator\n",
    "from sqlparse.tokens import DML, DDL, Keyword\n",
    "import sql_metadata as sqllib\n",
    "\n",
    "## Libreria que se integra con Teradata\n",
    "import giraffez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables Globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "noConditionInput = 'Parametros: No tiene condicion de tabla input'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones: Insertar en BD Teradata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuracion de conexion\n",
    "td_config = {\n",
    "    \"username\": \"exajibl\",\n",
    "    \"password\": \"acjb0610\",\n",
    "    \"host\": \"dataware.bci.cl\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_drop_tables_teradata():\n",
    "    '''\n",
    "    Función que crea las tablas en Teradata\n",
    "    '''\n",
    "    \n",
    "    print(\"ELIMINANDO Y CREANDO TABLAS..\")\n",
    "    drop_sql_bteq    = \"DROP TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_BTEQ\"\n",
    "    drop_sql_sp      = \"DROP TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_SP\"\n",
    "    drop_sql_vistas  = \"DROP TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_VISTAS\"\n",
    "    drop_sql_resumen = \"DROP TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_RESUMEN\"\n",
    "\n",
    "    create_sql_bteq = \"\"\"CREATE MULTISET TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_BTEQ\n",
    "        (\n",
    "          TIPO_ARCHIVO VARCHAR(20) ,\n",
    "          ARCHIVO VARCHAR(200),\n",
    "          NUMERO_PASO VARCHAR(5),      \n",
    "          SENTENCIA_DML VARCHAR(100) ,\n",
    "          ESQUEMA_OUTPUT VARCHAR(50) ,\n",
    "          TABLA_OUTPUT VARCHAR(100) ,\n",
    "          ESQUEMA_INPUT VARCHAR(50) ,\n",
    "          TABLA_INPUT VARCHAR(100)      \n",
    "          ) ;\"\"\"\n",
    "\n",
    "    create_sql_sp = \"\"\"CREATE MULTISET TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_SP\n",
    "        (\n",
    "          TIPO_ARCHIVO VARCHAR(20) ,\n",
    "          ARCHIVO VARCHAR(200),\n",
    "          NUMERO_PASO VARCHAR(3),      \n",
    "          SENTENCIA_DML VARCHAR(15) ,\n",
    "          ESQUEMA_OUTPUT VARCHAR(30) ,\n",
    "          TABLA_OUTPUT VARCHAR(30) ,\n",
    "          ESQUEMA_INPUT VARCHAR(50) ,\n",
    "          TABLA_INPUT VARCHAR(50)      \n",
    "          ) ;\"\"\"\n",
    "\n",
    "    create_sql_vistas = \"\"\"CREATE MULTISET TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_VISTAS\n",
    "        (\n",
    "          TIPO_ARCHIVO VARCHAR(20) ,\n",
    "          ARCHIVO VARCHAR(200),     \n",
    "          SENTENCIA_DML VARCHAR(15) ,\n",
    "          ESQUEMA_OUTPUT VARCHAR(30) ,\n",
    "          TABLA_OUTPUT VARCHAR(30) ,\n",
    "          ESQUEMA_INPUT VARCHAR(50) ,\n",
    "          TABLA_INPUT VARCHAR(50)      \n",
    "          ) ;\"\"\"\n",
    "\n",
    "    create_sql_resumen = \"\"\"CREATE MULTISET TABLE EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_RESUMEN\n",
    "        (\n",
    "          TIPO_ARCHIVO VARCHAR(20),\n",
    "          ARCHIVO VARCHAR(200),\n",
    "          TIPO_DE_TABLA VARCHAR(10),\n",
    "          ESQUEMA VARCHAR(30),\n",
    "          TABLA VARCHAR(50)\n",
    "          ) ;\"\"\"\n",
    "\n",
    "    with giraffez.Cmd(**td_config) as cmd:\n",
    "        if cmd.exists(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_BTEQ\"):\n",
    "            cmd.execute(drop_sql_bteq)\n",
    "        if cmd.exists(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_SP\"):\n",
    "            cmd.execute(drop_sql_sp)\n",
    "        if cmd.exists(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_VISTAS\"):\n",
    "            cmd.execute(drop_sql_vistas)\n",
    "        if cmd.exists(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_RESUMEN\"):\n",
    "            cmd.execute(drop_sql_resumen)\n",
    "\n",
    "\n",
    "        cmd.execute(create_sql_bteq)\n",
    "        cmd.execute(create_sql_sp)\n",
    "        cmd.execute(create_sql_vistas)\n",
    "        cmd.execute(create_sql_resumen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_files_csv_in_teradata():\n",
    "    '''\n",
    "    Función que inserta los dataframes a Teradata\n",
    "    '''\n",
    "    \n",
    "    print(\"INSERTANDO ARCHIVOS A LAS TABLAS..\\n\")\n",
    "\n",
    "    \n",
    "    df_bteq    = pd.read_csv(\"LAC_Levantamiento_Tablas_Bteq.csv\",  sep='|' ) \n",
    "    df_sp      = pd.read_csv(\"LAC_Levantamiento_Tablas_StoredProcedures.csv\" ,  sep='|') \n",
    "    df_vistas  = pd.read_csv(\"LAC_Levantamiento_Tablas_Vistas.csv\",  sep='|') \n",
    "    #df_resumen = pd.read_csv(\"LAC_Levantamiento_Tablas_Resumen.csv\",  sep='|' ) \n",
    "\n",
    "    \n",
    "    if (df_bteq.empty == False): \n",
    "        \n",
    "        print(\"INSERTANDO EN EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_BTEQ\")\n",
    "        \n",
    "        df_bteq['NUMERO_PASO'] = df_bteq['NUMERO_PASO'].astype('str')\n",
    "        \n",
    "        with giraffez.BulkLoad(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_BTEQ\", **td_config) as load:\n",
    "            load.cleanup()\n",
    "            load.columns = df_bteq.columns.tolist()\n",
    "            for row in df_bteq.values.tolist(): \n",
    "                load.put([row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7]])\n",
    "\n",
    "    \n",
    "    if (df_sp.empty == False):\n",
    "        \n",
    "        print(\"INSERTANDO EN EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_SP\")\n",
    "        \n",
    "        df_sp['TIPO_ARCHIVO']   = df_sp['TIPO_ARCHIVO'].astype('str')\n",
    "        df_sp['ARCHIVO']        = df_sp['ARCHIVO'].astype('str')\n",
    "        df_sp['NUMERO_PASO']    = df_sp['NUMERO_PASO'].astype('str')\n",
    "        df_sp['SENTENCIA_DML']  = df_sp['SENTENCIA_DML'].astype('str')\n",
    "        df_sp['ESQUEMA_OUTPUT'] = df_sp['ESQUEMA_OUTPUT'].astype('str')\n",
    "        df_sp['TABLA_OUTPUT']   = df_sp['TABLA_OUTPUT'].astype('str')\n",
    "        df_sp['ESQUEMA_INPUT']  = df_sp['ESQUEMA_INPUT'].astype('str')\n",
    "        df_sp['TABLA_INPUT']    = df_sp['TABLA_INPUT'].astype('str')\n",
    "        \n",
    "        \n",
    "        with giraffez.BulkLoad(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_SP\", **td_config) as load:\n",
    "            load.cleanup()\n",
    "            load.columns = df_sp.columns.tolist()\n",
    "            for row in df_sp.values.tolist(): \n",
    "                load.put([row[0], row[1], row[2], row[3], row[4], row[5], row[6], row[7]])  \n",
    "\n",
    "    if (df_vistas.empty == False): \n",
    "        \n",
    "        print(\"INSERTANDO EN EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_VISTAS\")\n",
    "        \n",
    "        \n",
    "        df_vistas['TIPO_ARCHIVO']   = df_vistas['TIPO_ARCHIVO'].astype('str')\n",
    "        df_vistas['ARCHIVO']        = df_vistas['ARCHIVO'].astype('str')\n",
    "        df_vistas['SENTENCIA_DML']  = df_vistas['SENTENCIA_DML'].astype('str')\n",
    "        df_vistas['ESQUEMA_OUTPUT'] = df_vistas['ESQUEMA_OUTPUT'].astype('str')\n",
    "        df_vistas['TABLA_OUTPUT']   = df_vistas['TABLA_OUTPUT'].astype('str')\n",
    "        df_vistas['ESQUEMA_INPUT']  = df_vistas['ESQUEMA_INPUT'].astype('str')\n",
    "        df_vistas['TABLA_INPUT']    = df_vistas['TABLA_INPUT'].astype('str')\n",
    " \n",
    "            \n",
    "        with giraffez.BulkLoad(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_VISTAS\", **td_config) as load:\n",
    "            load.cleanup()\n",
    "            load.columns = df_vistas.columns.tolist()\n",
    "            for row in df_vistas.values.tolist(): \n",
    "                load.put([row[0], row[1], row[2], row[3], row[4], row[5], row[6] ]) \n",
    "\n",
    "    '''\n",
    "    if (df_resumen.empty == False): \n",
    "        with giraffez.BulkLoad(\"EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_RESUMEN\", **td_config) as load:\n",
    "            load.cleanup()\n",
    "            load.columns = df_resumen.columns.tolist()\n",
    "            for row in df_resumen.values.tolist(): \n",
    "                load.put([row[0], row[1], row[2], row[3], row[4]]) \n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones modificadas de librería sql_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique(_list):\n",
    "    \"\"\"\n",
    "    Hace que una lista tenga registro unicos y mantengan el orden\n",
    "    \"\"\"\n",
    "    ret = []\n",
    "\n",
    "    for item in _list:\n",
    "        if item not in ret:\n",
    "            ret.append(item)\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_tables(file_type, filename, query):\n",
    "    \"\"\"\n",
    "    Función que retorna una lista de tablas INPUT de una query\n",
    "    \"\"\"\n",
    "    \n",
    "    tables              = []\n",
    "    tables_2            = []\n",
    "    table_list          = []\n",
    "    table_list_total    = []\n",
    "    last_keyword  = None\n",
    "    last_token    = None\n",
    "    dml_ddl_name  = None\n",
    "    not_get_from  = False\n",
    "    from_clausule = False\n",
    "\n",
    "\n",
    "    df_internal_table   = emptyDataframeTablesQuery(file_type)\n",
    "    \n",
    "    #     \n",
    "    dates_list = ['SECOND', 'MINUTE', 'HOUR', 'DAY', 'YEAR','MONTH','BOTH', 'TRAILING']\n",
    "    \n",
    "    stop_from_list = ['AS','CASE','WHEN','ON','AND','END','WHERE','GROUP','OVER','PARTITION','SET']\n",
    "\n",
    "    functions_ignored = ['COUNT', 'MIN', 'MAX', 'SUM', 'FROM_UNIXTIME', 'DEC', \n",
    "    'CAST', 'CONVERT', 'ZEROIFNULL','SUBSTR','SUBSTRING','ROW_NUMBER','QUALIFY', 'ADD_MONTHS',\n",
    "    'COALESCE', 'CHAR', 'INTEGER', 'TRIM', 'OVER', 'FORMAT', 'DATE_FORMAT',\n",
    "    'CHAR_LENGTH']\n",
    "\n",
    "    # Lista de funciones reservadas de SQL ANSI para las uniones de tablas\n",
    "    table_syntax_joins = ['FROM','JOIN', 'INNER JOIN', 'LEFT JOIN', 'LEFT OUTER JOIN', \n",
    "    'RIGHT JOIN', 'RIGHT OUTER JOIN']\n",
    "\n",
    "    table_syntax_keywords = [\n",
    "        # SELECT queries\n",
    "        'FROM', 'WHERE', 'JOIN', 'INNER JOIN', 'LEFT JOIN', 'LEFT OUTER JOIN', \n",
    "        'RIGHT JOIN', 'RIGHT OUTER JOIN', 'ON',\n",
    "        \n",
    "        # INSERT queries\n",
    "        'INTO', 'VALUES',\n",
    "        # UPDATE queries\n",
    "        'UPDATE', 'SET', \n",
    "        # Hive queries\n",
    "        'TABLE',  # INSERT TABLE\n",
    "    ]\n",
    "    \n",
    "    archivo_caido = 'SQLAExport1827.txt'\n",
    "    \n",
    "    for token in sqllib.get_query_tokens(query):\n",
    "        \n",
    "        token_value_clean = \" \".join(token.value.upper().split())\n",
    "        \n",
    "        ## Homologar en caso que se consiga un 'SEL' en vez de 'SELECT'\n",
    "        if ((token.ttype is Name and token.value.upper() == 'SEL')):\n",
    "            token.ttype = DML\n",
    "            token.value = 'SELECT'\n",
    "            last_keyword = 'SELECT'\n",
    "            \n",
    "        if (token.ttype is DML or token.ttype is DDL):\n",
    "            dml_ddl_name = token.value.upper()  \n",
    "            from_clausule = False      \n",
    "        \n",
    "        if token.ttype is Punctuation and token.value == ',' \\\n",
    "            and from_clausule == True:\n",
    "            last_keyword = 'FROM'\n",
    "        \n",
    "        if token.is_keyword and from_clausule == True and \\\n",
    "           token.value.upper() in stop_from_list:            \n",
    "            from_clausule = False\n",
    "            last_keyword = None\n",
    "            \n",
    "            \n",
    "        \n",
    "        ## OBTIENE TABLAS\n",
    "        '''\n",
    "        Sección que recupera las tablas de una query \n",
    "        Retorna una lista con el formato: [esquema, tabla, alias]\n",
    "        '''          \n",
    "        if token.is_keyword and token_value_clean in table_syntax_keywords:\n",
    "            # keep the name of the last keyword, the next one can be a table name\n",
    "            last_keyword = \" \".join(token.value.upper().split())\n",
    "            \n",
    "            ## Para los casos de algunos campos en el SELECT que llaman a FROM\n",
    "            if token.value.upper() == 'FROM' and last_token in dates_list:\n",
    "                not_get_from = True\n",
    "                \n",
    "            elif(token.value.upper() == 'FROM' and not_get_from == True):\n",
    "                not_get_from = False\n",
    "                \n",
    "            ## Para los casos de que las tablas estén separadas por ,    \n",
    "            elif(token.value.upper() == 'FROM' and last_token in [\"'\"+'0'+\"'\"]):\n",
    "                from_clausule = False\n",
    "                not_get_from  = True\n",
    "\n",
    "            elif(token.value.upper() == 'FROM' and last_token not in[\"'\"+'0'+\"'\"]):\n",
    "                from_clausule = True\n",
    "\n",
    "                \n",
    "            elif(token.value.upper() in ['WHERE','GROUP']):\n",
    "                from_clausule = False\n",
    "            \n",
    "        elif str(token) == '(' or str(token) == ')':\n",
    "            #print(\" ENTRO 2 \" )\n",
    "            # reset the last_keyword for INSERT `foo` VALUES(id, bar) ...\n",
    "            last_keyword = None\n",
    "        elif token.is_keyword and str(token) in ['FORCE', 'ORDER']:\n",
    "            #print(\" ENTRO 3 \" )\n",
    "            # reset the last_keyword for \"SELECT x FORCE INDEX\" queries and \"SELECT x ORDER BY\"\n",
    "            last_keyword = None\n",
    "        elif token.is_keyword and str(token) == 'SELECT' and last_keyword in ['INTO', 'TABLE']:\n",
    "            # reset the last_keyword for \"INSERT INTO SELECT\" and \"INSERT TABLE SELECT\" queries\n",
    "            last_keyword = None\n",
    "            #print(\" ENTRO 4 \" )\n",
    "        elif token.ttype is Name or token.is_keyword:\n",
    "            # print([last_keyword, last_token, token.value])\n",
    "            # analyze the name tokens, column names and where condition values\n",
    "\n",
    "            if token.ttype is Name and token.value in functions_ignored \\\n",
    "               and from_clausule == True :\n",
    "                not_get_from = True\n",
    "\n",
    "\n",
    "            if last_keyword in ['FROM', 'JOIN', 'INNER JOIN', 'LEFT JOIN', 'RIGHT JOIN',\n",
    "                                'LEFT OUTER JOIN','RIGHT OUTER JOIN', 'UPDATE' ] \\\n",
    "                    and last_token not in ['AS', 'CREATE'] \\\n",
    "                    and token.value not in ['AS', 'SELECT'] \\\n",
    "                    and not_get_from == False:\n",
    "\n",
    "                if last_token == '.':\n",
    "                    \n",
    "                    # we have database.table notation example\n",
    "                    # append table name to the last entry of tables\n",
    "                    # as it is a database name in fact\n",
    "                    database_name = tables[-1]\n",
    "                    table = token.value\n",
    "                    tables[-1] = '{}.{}'.format(database_name, token)\n",
    "                    last_keyword = None\n",
    "                    \n",
    "                    table_list = [database_name, table]\n",
    "                    if (dml_ddl_name not in ['DROP']):\n",
    "                        table_list_total.append(table_list)\n",
    "\n",
    "\n",
    "                elif last_token not in [',', last_keyword] and \\\n",
    "                    last_keyword not in table_syntax_joins:\n",
    "                    # it's not a list of tables, e.g. SELECT * FROM foo, bar\n",
    "                    # hence, it can be the case of alias without AS, e.g. SELECT * FROM foo bar\n",
    "                    pass\n",
    "                else: #esquema                         \n",
    "                    table_name = str(token.value.strip('`'))\n",
    "                    tables.append(table_name)\n",
    "                \n",
    "                            \n",
    "        last_token = token.value.upper()\n",
    "\n",
    "    \n",
    "    return unique(table_list_total)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de procesamiento de query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_comments(text):\n",
    "    \"\"\" remove c-style comments.\n",
    "        text: blob of text with comments (can include newlines)\n",
    "        returns: text with comments removed\n",
    "    \"\"\"\n",
    "    pattern = r\"\"\"\n",
    "                            ##  --------- COMMENT ---------\n",
    "           /\\*              ##  Start of /* ... */ comment\n",
    "           [^*]*\\*+         ##  Non-* followed by 1-or-more *'s\n",
    "           (                ##\n",
    "             [^/*][^*]*\\*+  ##\n",
    "           )*               ##  0-or-more things which don't start with /\n",
    "                            ##    but do end with '*'\n",
    "           /                ##  End of /* ... */ comment\n",
    "         |                  ##  -OR-  various things which aren't comments:\n",
    "           (                ## \n",
    "                            ##  ------ \" ... \" STRING ------\n",
    "             \"              ##  Start of \" ... \" string\n",
    "             (              ##\n",
    "               \\\\.          ##  Escaped char\n",
    "             |              ##  -OR-\n",
    "               [^\"\\\\]       ##  Non \"\\ characters\n",
    "             )*             ##\n",
    "             \"              ##  End of \" ... \" string\n",
    "           |                ##  -OR-\n",
    "                            ##\n",
    "                            ##  ------ ' ... ' STRING ------\n",
    "             '              ##  Start of ' ... ' string\n",
    "             (              ##\n",
    "               \\\\.          ##  Escaped char\n",
    "             |              ##  -OR-\n",
    "               [^'\\\\]       ##  Non '\\ characters\n",
    "             )*             ##\n",
    "             '              ##  End of ' ... ' string\n",
    "           |                ##  -OR-\n",
    "                            ##\n",
    "                            ##  ------ ANYTHING ELSE -------\n",
    "             .              ##  Anything other char\n",
    "             [^/\"'\\\\]*      ##  Chars which doesn't start a comment, string\n",
    "           )                ##    or escape\n",
    "    \"\"\"\n",
    "    regex = re.compile(pattern, re.VERBOSE|re.MULTILINE|re.DOTALL)\n",
    "    noncomments = [m.group(2) for m in regex.finditer(text) if m.group(2)]\n",
    "\n",
    "    return \"\".join(noncomments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tables_output(query):\n",
    "    ''' \n",
    "    Recorre los tokens de 'INSERT, REPLACE VIEW, CREATE' hasta conseguir un 'SELECT'\n",
    "    Retorna lista: [tipo de STMT, esquema output, tabla output]\n",
    "          Ejemplo: ['CREATE TABLE', 'ARMWORK','RM_BANCA_A']\n",
    "    '''\n",
    "    list_output = []\n",
    "    create_seen         = False\n",
    "    replace_seen        = False\n",
    "    rename_seen         = False\n",
    "    drop_seen           = False\n",
    "    last_token          = None\n",
    "\n",
    "    for token in sqllib.get_query_tokens(query):\n",
    "        \n",
    "        if (token.ttype is DML and token.value.upper() == 'SELECT') \\\n",
    "        or (token.ttype is Punctuation and token.value in ['(',';']) \\\n",
    "        or (token.ttype is Keyword and token.value == 'VALUES') :\n",
    "            return list_output\n",
    "            \n",
    "        elif token.ttype is DML and \\\n",
    "            token.value.upper() in ['INSERT','UPDATE', 'DELETE'] :\n",
    "            list_output.append(token.value.upper())\n",
    "        \n",
    "        elif (last_token == 'DELETE' and token.value.upper() != 'FROM' and len(token.value) <= 3):\n",
    "            pass\n",
    "        \n",
    "        elif (last_token == 'UPDATE' and len(token.value) <= 3):\n",
    "            pass\n",
    "            \n",
    "        elif token.ttype is DDL and token.value.upper() == 'CREATE':\n",
    "            token_str = 'CREATE'\n",
    "            create_seen = True\n",
    "\n",
    "        elif token.is_keyword and token.value.upper() == 'RENAME':\n",
    "            rename_seen = True            \n",
    "  \n",
    "        elif token.is_keyword and token.value.upper() == 'TABLE' and rename_seen:\n",
    "            token_str = 'RENAME TABLE'\n",
    "            list_output.append(token_str)            \n",
    "            \n",
    "        elif token.ttype is DDL and token.value.upper() == 'DROP':\n",
    "            token_str = 'DROP'\n",
    "            drop_seen = True\n",
    "            list_output.append(token_str)\n",
    "\n",
    "        elif token.is_keyword and token.value.upper() == 'JOIN' and drop_seen:\n",
    "            token_str = 'DROP JOIN INDEX'\n",
    "            list_output[-1] = token_str\n",
    "        \n",
    "        elif token.is_keyword and token.value.upper() == 'INDEX' and drop_seen:\n",
    "            token_str = token_str + 'INDEX'\n",
    "            list_output[-1] = token_str\n",
    "            \n",
    "        elif token.is_keyword and token.value.upper() == 'INDEX' and create_seen:\n",
    "            list_output.append('CREATE INDEX')\n",
    " \n",
    "        elif token.is_keyword and token.value.upper() == 'JOIN' and create_seen:\n",
    "            token_str = 'CREATE JOIN INDEX'\n",
    "            create_seen = False\n",
    "            list_output.append(token_str)\n",
    "\n",
    "        elif token.ttype is Name and token.value.upper() == 'SET' and create_seen:\n",
    "            token_str = 'CREATE SET TABLE'\n",
    "            create_seen = False\n",
    "            list_output.append(token_str)\n",
    "\n",
    "        elif token.ttype is Name and token.value.upper() == 'MULTISET' and create_seen:\n",
    "            token_str = 'CREATE MULTISET TABLE'\n",
    "            create_seen = False\n",
    "            list_output.append(token_str)\n",
    "            \n",
    "            '''    \n",
    "            elif token.ttype is Name and token.value.upper() == 'VOLATILE' and create_seen:\n",
    "                token_str = 'CREATE MULTISET VOLATILE TABLE'\n",
    "                create_seen = False\n",
    "                list_output.append(token_str)\n",
    "            '''    \n",
    "            \n",
    "        elif token.is_keyword and token.value.upper() == 'TABLE' and create_seen:\n",
    "            token_str = 'CREATE TABLE'\n",
    "            list_output.append(token_str)\n",
    "\n",
    "        elif token.is_keyword and token.value.upper() == 'VIEW' and create_seen:\n",
    "            token_str = 'CREATE VIEW'\n",
    "            list_output.append(token_str)\n",
    "        \n",
    "        elif token.ttype is DML and token.value.upper() == 'REPLACE':\n",
    "            replace_seen = True\n",
    "            \n",
    "        elif token.is_keyword and token.value.upper() == 'VIEW' and replace_seen:\n",
    "            token_str = 'REPLACE VIEW'\n",
    "            list_output.append(token_str)\n",
    "        \n",
    "        elif token.ttype is Name and token.value == 'HASH':\n",
    "            token_str = token_str + ' HASH '\n",
    "            \n",
    "        elif token.ttype is Name and token.value not in ['HASH']:\n",
    "            list_output.append(token.value)\n",
    "            \n",
    "        \n",
    "        last_token = token.value.upper() \n",
    "                   \n",
    "    return 'DESCONOCIDO'    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteInconsistencyQuery(queryString):\n",
    "    '''\n",
    "    Función que elimina caracteres varios de una query\n",
    "    '''    \n",
    "    characters = '[%#${}]'\n",
    "    finalQuery = re.sub(characters, '', str(queryString))\n",
    "    return finalQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_query(queryString):\n",
    "    '''\n",
    "    Función que limpia y ordena una query para ser procesada posteriormente\n",
    "    '''\n",
    "    queryString = queryString.replace('\"','')\n",
    "    queryString = sqllib.preprocess_query(queryString)\n",
    "    queryString = deleteInconsistencyQuery(queryString)\n",
    "    return queryString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_queries(queryString):\n",
    "    '''\n",
    "    Función que procesa una query extrayendo solamente su tabla ouput\n",
    "    '''\n",
    "    listaQuery = []\n",
    "    list_syntax_large = ['CREATE MULTISET TABLE', 'DROP JOIN INDEX']\n",
    "    \n",
    "    if (queryString):\n",
    "\n",
    "        parsed                  = sqlparse.parse(queryString)[0]\n",
    "        tipo_DML_DDL_principal  = parsed.token_first().value.upper()\n",
    "        lista_tabla_output      = get_tables_output(queryString)\n",
    "            \n",
    "        if (lista_tabla_output[0] in ['DROP']):\n",
    "            tabla_completa  = sqllib.get_query_tables(queryString)[0]             \n",
    "            base_datos      = tabla_completa.split('.')[0]\n",
    "            tabla           = tabla_completa.split('.')[1]\n",
    "            \n",
    "            listaQuery.append(tipo_DML_DDL_principal)\n",
    "            listaQuery.append(base_datos) \n",
    "            listaQuery.append(tabla) \n",
    "            \n",
    "        else:\n",
    "            return lista_tabla_output\n",
    "\n",
    "    \n",
    "    return listaQuery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_output_tables(folder, filename, queryString):\n",
    "    '''\n",
    "    Función que procesa una query retornando su tabla output y la lista de tablas input\n",
    "    Si no existen tablas input, se coloca un mensaje que indica que no tiene condiciones\n",
    "    '''\n",
    "    list_table_input  = []\n",
    "    list_table_output = []\n",
    "\n",
    "    list_syntax_large = ['CREATE MULTISET TABLE', 'DROP JOIN INDEX']\n",
    "    \n",
    "    if (queryString):\n",
    "\n",
    "        parsed                  = sqlparse.parse(queryString)[0]\n",
    "        type_DML_DDL_principal  = parsed.token_first().value.upper()\n",
    "        list_table_output       = get_tables_output(queryString)\n",
    "                \n",
    "        if (type_DML_DDL_principal in ['DELETE', 'UPDATE']):\n",
    "                        \n",
    "            list_table_input = get_query_tables(folder, filename, queryString) \n",
    "            \n",
    "            if (list_table_input):\n",
    "                output_table     = list_table_input[0]\n",
    "\n",
    "                list_table_output.clear()            \n",
    "                list_table_output.append(type_DML_DDL_principal)\n",
    "                list_table_output.append(output_table[0])   \n",
    "                list_table_output.append(output_table[1])  \n",
    "\n",
    "                list_table_input.remove(output_table)\n",
    "                            \n",
    "        elif (list_table_output[0] in ['DROP']):\n",
    "            output_table    = sqllib.get_query_tables(queryString)[0] \n",
    "            l_output_table  = output_table.split('.')\n",
    "            len_l_output_table = len(l_output_table)\n",
    "\n",
    "            # Para los casos que hay tablas volatiles sin esquema\n",
    "            if (len_l_output_table == 2):\n",
    "                database    = l_output_table[0]\n",
    "                table       = l_output_table[1]\n",
    "\n",
    "                list_table_output.clear() \n",
    "                list_table_output.append(type_DML_DDL_principal)\n",
    "                list_table_output.append(database) \n",
    "                list_table_output.append(table) \n",
    "\n",
    "            elif (len_l_output_table == 1):\n",
    "                table       = l_output_table[0]\n",
    "\n",
    "                list_table_output.clear() \n",
    "                list_table_output.append(type_DML_DDL_principal)\n",
    "                list_table_output.append('N/A') \n",
    "                list_table_output.append(table) \n",
    "            \n",
    "        else:\n",
    "            list_table_input = get_query_tables(folder, filename, queryString) \n",
    "\n",
    "    ## Si no tiene tablas, agregar info de no tener condicion de tablas input\n",
    "    if (len(list_table_input) == 0): \n",
    "        result = [ noConditionInput, noConditionInput ]\n",
    "        list_table_input.append(result)\n",
    "    \n",
    "    return list_table_input, list_table_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones: Insertar en Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emptyDataframeTablesQuery(file_type):\n",
    "    '''\n",
    "    Función que retorna un objeto dataframe vacío para obtener las tablas, campos y reglas de una query    \n",
    "    Formato ['TIPO_ARCHIVO','ARCHIVO','NUMERO_PASO','SENTENCIA_DML','ESQUEMA_OUTPUT','TABLA_OUTPUT',\n",
    "            'ESQUEMA_INPUT','TABLA_INPUT']\n",
    "    '''\n",
    "    \n",
    "    columnsDF = []\n",
    "\n",
    "    if (file_type in ['Bteq','StoredProcedures']):\n",
    "        columnsDF = [\n",
    "            'TIPO_ARCHIVO','ARCHIVO', \n",
    "            'NUMERO_PASO','SENTENCIA_DML',\n",
    "            'ESQUEMA_OUTPUT','TABLA_OUTPUT',\n",
    "            'ESQUEMA_INPUT','TABLA_INPUT'\n",
    "        ]\n",
    "\n",
    "    elif (file_type == 'Vistas'):  \n",
    "        columnsDF = [\n",
    "            'TIPO_ARCHIVO','ARCHIVO', \n",
    "            'SENTENCIA_DML',\n",
    "            'ESQUEMA_OUTPUT','TABLA_OUTPUT',\n",
    "            'ESQUEMA_INPUT','TABLA_INPUT'\n",
    "        ] \n",
    "\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame(columns = columnsDF) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emptyDataframeTablesSummary(data_list):\n",
    "    '''\n",
    "    Función que retorna un objeto dataframe vacío \n",
    "    para obtener las tablas (input, process y output) de un archivo procesado\n",
    "    \n",
    "    Formato ['TIPO_ARCHIVO','ARCHIVO','TABLA_INPUT','TABLA_PROCESS','TABLA_OUTPUT']\n",
    "    '''\n",
    "    \n",
    "    columnsDF = ['TIPO_ARCHIVO','ARCHIVO','TIPO_DE_TABLA','ESQUEMA','TABLA']\n",
    "    \n",
    "    if (data_list is not None):\n",
    "        df = pd.DataFrame(data_list, columns = columnsDF) \n",
    "    else:        \n",
    "        df = pd.DataFrame(columns = columnsDF) \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insertDataframeTablesQuery(file_type,file,process_number,output_table,input_tables_list):\n",
    "    '''\n",
    "    Función que inserta en un dataframe los objetos de una query\n",
    "    '''\n",
    "\n",
    "    new_df_tables = emptyDataframeTablesQuery(file_type)\n",
    "        \n",
    "    #print (file +\"  *** LISTA TABLAS INPUT -> \"  + str(input_tables_list))  \n",
    "    #print (file +\"  *** LISTA TABLAS OUTPUT -> \"  + str(output_table)) \n",
    " \n",
    "    if (file_type in ['Bteq','StoredProcedures']):\n",
    "        if (input_tables_list):\n",
    "            for item in input_tables_list:\n",
    "                \n",
    "                if (len(output_table) == 3):\n",
    "                    new_df_tables = new_df_tables.append(\n",
    "                        [ {'TIPO_ARCHIVO'    : file_type, \n",
    "                            'ARCHIVO'        : file,\n",
    "                            'NUMERO_PASO'    : process_number,\n",
    "                            'SENTENCIA_DML'  : output_table[0],\n",
    "                            'ESQUEMA_OUTPUT' : output_table[1],\n",
    "                            'TABLA_OUTPUT'   : output_table[2],\n",
    "                            'ESQUEMA_INPUT'  : item[0],\n",
    "                            'TABLA_INPUT'    : item[1] }], \n",
    "                            ignore_index=True, sort=False) \n",
    "        \n",
    "        elif (output_table[0] == 'RENAME TABLE' \\\n",
    "         and len(output_table) == 5):\n",
    "            new_df_tables = new_df_tables.append(\n",
    "                [ {'TIPO_ARCHIVO'    : file_type, \n",
    "                    'ARCHIVO'        : file,\n",
    "                    'NUMERO_PASO'    : process_number,\n",
    "                    'SENTENCIA_DML'  : output_table[0],\n",
    "                    'ESQUEMA_OUTPUT' : output_table[3],\n",
    "                    'TABLA_OUTPUT'   : output_table[4],\n",
    "                    'ESQUEMA_INPUT'  : output_table[1],\n",
    "                    'TABLA_INPUT'    : output_table[2] }], \n",
    "                    ignore_index=True, sort=False) \n",
    "            \n",
    "        \n",
    "        else:\n",
    "            new_df_tables = new_df_tables.append(\n",
    "                [ {'TIPO_ARCHIVO'    : file_type, \n",
    "                    'ARCHIVO'        : file,\n",
    "                    'NUMERO_PASO'    : process_number,\n",
    "                    'SENTENCIA_DML'  : output_table[0],\n",
    "                    'ESQUEMA_OUTPUT' : output_table[1],\n",
    "                    'TABLA_OUTPUT'   : output_table[2],\n",
    "                    'ESQUEMA_INPUT'  : '',\n",
    "                    'TABLA_INPUT'    : '' }], \n",
    "                    ignore_index=True, sort=False) \n",
    "            \n",
    "    elif (file_type == 'Vistas'):\n",
    "        for item in input_tables_list:\n",
    "            new_df_tables = new_df_tables.append(\n",
    "                [ {'TIPO_ARCHIVO'    : file_type, \n",
    "                    'ARCHIVO'        : file,\n",
    "                    'SENTENCIA_DML'  : output_table[0],\n",
    "                    'ESQUEMA_OUTPUT' : output_table[1],\n",
    "                    'TABLA_OUTPUT'   : output_table[2],\n",
    "                    'ESQUEMA_INPUT'  : item[0],\n",
    "                    'TABLA_INPUT'    : item[1] }], \n",
    "                    ignore_index=True, sort=False)  \n",
    "        \n",
    "    return new_df_tables\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returnSummaryDataframeTables(df, file_type):\n",
    "    '''\n",
    "    Función que genera un resumen de que tipo de objeto es cada tabla dentro de una query\n",
    "    Tipos de objetos: OUTPUT, INPUT, PROCESO\n",
    "    '''\n",
    "\n",
    "    df_summary      = emptyDataframeTablesSummary(None)\n",
    "    process_list    = ['DROP']\n",
    "    input_list      = ['INSERT']\n",
    "    output_list     = ['RENAME VIEW','CREATE VIEW','REPLACE VIEW'] \n",
    "    columns_output  = ['TIPO_ARCHIVO','ARCHIVO','TABLA_OUTPUT','ESQUEMA_OUTPUT']\n",
    "    columns_input   = ['TIPO_ARCHIVO','ARCHIVO','TABLA_INPUT','ESQUEMA_INPUT']\n",
    "    df_process      = pd.DataFrame(columns = columns_output)\n",
    "    df_input        = pd.DataFrame(columns = columns_input)\n",
    "    df_output       = pd.DataFrame(columns = columns_output)\n",
    "\n",
    "\n",
    "    if (df.empty == False): \n",
    "\n",
    "        if (file_type == 'Vistas'):\n",
    "\n",
    "            df_input  = df[columns_input].groupby([\"TIPO_ARCHIVO\", \"ARCHIVO\",\"TABLA_INPUT\",\"ESQUEMA_INPUT\"]).\\\n",
    "                        last().reset_index()\n",
    "                    \n",
    "            df_output = df[columns_output].groupby([\"TIPO_ARCHIVO\", \"ARCHIVO\",\"TABLA_OUTPUT\",\"ESQUEMA_OUTPUT\"]).\\\n",
    "                        last().reset_index()            \n",
    "\n",
    "\n",
    "        elif (file_type in ['Bteq','StoredProcedures']): ## obtener el ultimo INSERT de los pasos\n",
    "\n",
    "            \n",
    "            ## Tablas que son PROCESO: Obtiene las tablas que se destruyen.\n",
    "            filter_df = df.loc[df['SENTENCIA_DML'].isin(process_list)]\n",
    "\n",
    "            df_process = filter_df[columns_output].\\\n",
    "                   groupby([\"TIPO_ARCHIVO\", \"ARCHIVO\",\"TABLA_OUTPUT\",\"ESQUEMA_OUTPUT\"]).last().reset_index()\n",
    "\n",
    "            df_process['TIPO_DE_TABLA'] = 'PROCESO'\n",
    "\n",
    "            df_process = df_process.rename(columns={'TABLA_OUTPUT': 'TABLA','ESQUEMA_OUTPUT': 'ESQUEMA'})\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ## Tablas que son INPUT: Obtiene las tablas que se encuentran en un SELECT.\n",
    "            filter_df = df.loc[df['SENTENCIA_DML'].isin(input_list)]\n",
    "\n",
    "            ## para que no retorne el mensaje de cuando no tiene tablas como input\n",
    "            filter   = ~filter_df['TABLA_INPUT'].str.contains(noConditionInput)\n",
    "\n",
    "            df_input = filter_df[columns_input].where(filter).\\\n",
    "                         groupby([\"TIPO_ARCHIVO\", \"ARCHIVO\",\"TABLA_INPUT\",\"ESQUEMA_INPUT\"]).last().reset_index()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            ## Tablas que son OUTPUT: Obtiene la última tabla que fue insertada en el proceso.\n",
    "            filter_df = df.loc[df['SENTENCIA_DML'].isin(input_list)]  \n",
    "\n",
    "            filter    = ~filter_df['TABLA_OUTPUT'].str.contains('LOG') ## descartar las tablas LOG\n",
    "\n",
    "            filter_df = filter_df.where(filter).dropna()\n",
    "\n",
    "            df_output = filter_df[-1:]  \n",
    "\n",
    "            df_output = df_output[columns_output]\n",
    "\n",
    "\n",
    "        ## Agrega campo de tipo de tabla y modifica los nombre estandar    \n",
    "        df_input['TIPO_DE_TABLA'] = 'INPUT'\n",
    "        df_input  = df_input.rename(columns={'TABLA_INPUT': 'TABLA','ESQUEMA_INPUT': 'ESQUEMA'})\n",
    "\n",
    "        df_output['TIPO_DE_TABLA'] = 'OUTPUT'\n",
    "        df_output = df_output.rename(columns={'TABLA_OUTPUT': 'TABLA','ESQUEMA_OUTPUT': 'ESQUEMA'})\n",
    "\n",
    "      \n",
    "    \n",
    "        ## Inserta resultados en dataframe resumen\n",
    "        if (df_process.empty == False): \n",
    "            \n",
    "            df_process = df_process.drop_duplicates()\n",
    "            df_summary = df_summary.append(df_process,ignore_index=True,sort=False)\n",
    "            \n",
    "            \n",
    "        if (df_input.empty == False):\n",
    "            \n",
    "            df_input   = df_input.drop_duplicates()\n",
    "            df_summary = df_summary.append(df_input,ignore_index=True,sort=False)\n",
    "                \n",
    "                \n",
    "        if (df_output.empty == False):\n",
    "            \n",
    "            df_output  = df_output.drop_duplicates()\n",
    "            df_summary = df_summary.append(df_output,ignore_index=True,sort=False)\n",
    "\n",
    "            \n",
    "            \n",
    "    return df_summary\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de procesamiento de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_files_csv():\n",
    "    '''\n",
    "    Función que elimina del directorio local los archivos antiguos de levantamiento\n",
    "    '''\n",
    "    for filename in glob.glob(\"./LAC_Levantamiento_Tablas_*.csv\"):\n",
    "        os.remove(filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_scandir_files(path):\n",
    "    '''\n",
    "    Función que escanea los directorios y subdirectorios,\n",
    "    obteniendo como resultado una lista de archivo con,\n",
    "    formato: ./Directorio/Subdirectorio/archivo\n",
    "    '''\n",
    "\n",
    "    file_list = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if(file.endswith(\".sql\") or file.endswith('.txt')):\n",
    "                file_list.append(os.path.join(root,file))\n",
    "                \n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_contains_name(folder_dir):\n",
    "    '''\n",
    "    Función que retorna que tipo de archivo se está procesando\n",
    "    '''\n",
    "    \n",
    "    if (folder_dir.find('Bteq') != -1 or folder_dir.find('BTEQ') != -1):\n",
    "        return 'Bteq'\n",
    "    if (folder_dir.find('Vistas') != -1):\n",
    "        return 'Vistas'\n",
    "    if (folder_dir.find('StoredProcedures') != -1):\n",
    "        return 'StoredProcedures'\n",
    "    \n",
    "    return 'Bteq'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_file_errors(MyFile, file_type, archivo_no_procesado):\n",
    "    '''\n",
    "    Función que inserta en un archivo que por alguna razón da error\n",
    "    '''\n",
    "    MyFile.write(\"%s\" % 'Archivo tipo *'+ str(file_type) + '*: '+archivo_no_procesado+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_dataframe_in_csv(df, file_type):\n",
    "    '''\n",
    "    Función que crea un archivo de levantamiento desde un dataframe\n",
    "    '''\n",
    "    \n",
    "    filename = 'LAC_Levantamiento_Tablas_'+file_type+'.csv'\n",
    "    \n",
    "    with open(filename, 'a') as f:\n",
    "        df.to_csv(f, sep='|', index=None, header=f.tell()==0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_summary_dataframe_in_csv(df): \n",
    "    '''\n",
    "    Función que crea un archivo de resumen de levantamiento desde un dataframe \n",
    "    ''' \n",
    "    \n",
    "    filename = 'LAC_Levantamiento_Tablas_Resumen.csv'\n",
    "    \n",
    "    with open(filename, 'a') as f:\n",
    "        df.to_csv(f, sep='|', index=None, header=f.tell()==0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_files(folder, filename):\n",
    "    '''\n",
    "    Función que abre un archivo y lo procesa linea por linea, eliminando sus comentarios \n",
    "    y retornando las querys a procesar en una lista\n",
    "    '''\n",
    "\n",
    "    queryVista           = ''    \n",
    "    \n",
    "    ## Lista de sintaxis SQL que debería tomar cada linea del archivo,\n",
    "    ## contiene espacio al final porque hay campos que pueden llamarse \"Update_dttm\"\n",
    "    ## y tomarlo como una palabra reservada\n",
    "    dml_dll_syntax_query = [\n",
    "        'INSERT', 'CREATE ', \n",
    "        'UPDATE ', \n",
    "        'DROP ', 'DELETE ','DELETE',\n",
    "        'RENAME '\n",
    "    ]\n",
    "    \n",
    "    if os.path.exists(filename):\n",
    "        \n",
    "        archivo         = open(filename, encoding=\"utf8\",errors='ignore')\n",
    "        textoArchivo    = archivo.read()\n",
    "        listaQuery      = []\n",
    "        sum_lines       = ''\n",
    "        \n",
    "        ## BTEQ Y STORED PROCEDURES \n",
    "        if (folder in ['Bteq','StoredProcedures']):\n",
    "            \n",
    "            bteq_text = re.sub('\\-\\-.*?\\n|\\/\\*.*?\\*\\/', ' ', textoArchivo)  \n",
    "                                    \n",
    "            bteq_text = remove_comments(bteq_text)\n",
    "                        \n",
    "            lines = bteq_text.splitlines();\n",
    "            \n",
    "            for line in lines: \n",
    "                # elimina espacios a comienzo de la linea y lo edita a mayuscula\n",
    "                line = line.strip().upper()                \n",
    "                                    \n",
    "                # comienza agregando las querys que interesan\n",
    "                if line.startswith(tuple(dml_dll_syntax_query)) and not line.startswith('CREATE INDEX'): \n",
    "                    sum_lines = line\n",
    "                    if line.endswith(\";\"):\n",
    "                        listaQuery.append(line)\n",
    "                        sum_lines = ''                        \n",
    "                        \n",
    "                # insertar en lista cuando termina la query\n",
    "                elif (line.endswith(\";\") and sum_lines != ''): \n",
    "                    sum_lines = sum_lines + ' '+ line\n",
    "                    listaQuery.append(sum_lines)  \n",
    "                    sum_lines = ''      \n",
    "                    \n",
    "                # acumula en una variable hasta conseguir un ;\n",
    "                elif (sum_lines != ''):                \n",
    "                    sum_lines = sum_lines + ' '+ line\n",
    "                           \n",
    "        ## VIEWS\n",
    "        elif (folder == 'Vistas'):\n",
    "            # LIMPIA EL CODIGO SQL PARA QUE NO TENGA NINGUN TIPO DE COMENTARIO\n",
    "            queryVista = re.sub('\\-\\-.*?\\n|\\/\\*.*?\\*\\/', ' ', textoArchivo)   \n",
    "\n",
    "            queryVista = remove_comments(queryVista)\n",
    "            \n",
    "            queryVista = queryVista.replace('\\\\', '')  \n",
    "            queryVista = queryVista.replace('\"', '') \n",
    "            \n",
    "            listaQuery.append(queryVista)\n",
    "   \n",
    "    return listaQuery\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_folders_files(ppal_dir):   \n",
    "    '''\n",
    "    Función que itera en los directorios para obtener los archivos que se \n",
    "    quieren procesar (bteq, sp, vistas)\n",
    "    '''\n",
    "\n",
    "    MyFile=open('archivos_no_procesados.txt','w')\n",
    "    \n",
    "    ## Elimina los archivos de levantamiento\n",
    "    remove_files_csv()\n",
    "    \n",
    "    folder_dir       = None      # Ruta completa de cada carpeta a leer\n",
    "    result_folder    = None \n",
    "    \n",
    "    # para pruebas\n",
    "    filename2        = 'SQLAExport1827.txt'\n",
    "    \n",
    "    ## Obtiene los nombres de los archivo de extracción con sus carpetas y subcarpetas.    \n",
    "    folder_list = fast_scandir_files(ppal_dir)    \n",
    "    folder_list.sort()\n",
    "\n",
    "    \n",
    "    for folder_file in folder_list:\n",
    "        \n",
    "        ## Retorna nombre corto del tipo de archivo que se está procesando (Bteq, Vista, StoredProcedure)\n",
    "        result_folder = folder_contains_name(folder_file)\n",
    "        \n",
    "        if result_folder is not None:\n",
    "            \n",
    "            print(folder_file)\n",
    "\n",
    "            ## inserta en un dataframe el contenido por query                       \n",
    "            df_internal_table      = emptyDataframeTablesQuery(result_folder)\n",
    "\n",
    "            ## inserta en un dataframe el contenido por archivo completo                    \n",
    "            df_internal_table_file = emptyDataframeTablesQuery(result_folder)\n",
    "\n",
    "            ## insertar en un dataframe resumen las tablas por Input, Proceso, Output\n",
    "            df_internal_summary    = emptyDataframeTablesSummary(None)\n",
    "\n",
    "            listaQuerys = processing_files(result_folder, folder_file)\n",
    "\n",
    "            if(len(listaQuerys) == 0):\n",
    "                insert_file_errors(MyFile, result_folder, folder_file)\n",
    "\n",
    "            process_number = 0\n",
    "\n",
    "            for queryString in listaQuerys:\n",
    "\n",
    "                process_number += 1\n",
    "\n",
    "                if (queryString):\n",
    "                    #print(queryString)\n",
    "                    try:\n",
    "                        queryString    = preprocess_query(queryString)\n",
    "\n",
    "\n",
    "                        # Retorna todas las tablas input y output en listas\n",
    "                        input_tables, output_tables = get_input_output_tables(result_folder, folder_file, queryString)\n",
    "\n",
    "                        # Retorna todas las tablas input y output en dataframe\n",
    "                        df_internal_table = insertDataframeTablesQuery(result_folder, folder_file,\\\n",
    "                                          process_number, output_tables, input_tables)\n",
    "\n",
    "\n",
    "                        if (df_internal_table.empty == False):\n",
    "                            insert_dataframe_in_csv(df_internal_table, result_folder)\n",
    "                            df_internal_table_file = df_internal_table_file.append(df_internal_table)\n",
    "                        else:\n",
    "                            insert_file_errors(MyFile, result_folder, folder_file)   \n",
    "                    \n",
    "                    \n",
    "                    except Exception as e: \n",
    "                        print(\"Error with file: \"+folder_file)\n",
    "                        print(e)\n",
    "                        insert_file_errors(MyFile, result_folder, folder_file)\n",
    "                        pass  \n",
    "                      \n",
    "            '''\n",
    "            df_internal_summary    = returnSummaryDataframeTables(df_internal_table_file, result_folder)\n",
    "\n",
    "            if (df_internal_summary.empty == False):\n",
    "                insert_summary_dataframe_in_csv(df_internal_summary)\n",
    "            '''\n",
    "                          \n",
    "    MyFile.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-11 18:48:13.301321\n",
      "INSERTANDO ARCHIVOS A LAS TABLAS..\n",
      "\n",
      "INSERTANDO EN EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_SP\n",
      "INSERTANDO EN EDW_TEMPUSU.LAC_LEVANTAMIENTO_TABLAS_VISTAS\n",
      "2020-05-11 18:49:18.978045\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    now = datetime.datetime.now()\n",
    "    print (now)\n",
    "    \n",
    "    # Ruta principal donde comienza la extracción\n",
    "    ppal_dir        = './ArchivosProcesar'       \n",
    "    \n",
    "    ## Recorre todas las carpetas y subcarpetas para extraer los archivos     \n",
    "    loop_folders_files(ppal_dir)\n",
    "    \n",
    "    # Elimina y crea las tablas en Teradata\n",
    "    create_and_drop_tables_teradata()\n",
    "    \n",
    "    # Inserta el contenido de los archivos a las tablas en Teradata\n",
    "    insert_files_csv_in_teradata()\n",
    "    \n",
    "    now = datetime.datetime.now()\n",
    "    print (now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
